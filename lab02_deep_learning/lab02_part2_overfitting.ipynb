{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFmOh482SyEF"
      },
      "source": [
        "## Lab 2\n",
        "### Part 2: Dealing with overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjzAuO3oSvsI"
      },
      "source": [
        "Today we work with [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist) (*hint: it is available in `torchvision`*).\n",
        "\n",
        "Your goal for today:\n",
        "1. Train a FC (fully-connected) network that achieves >= 0.885 test accuracy.\n",
        "2. Cause considerable overfitting by modifying the network (e.g. increasing the number of network parameters and/or layers) and demonstrate in in the appropriate way (e.g. plot loss and accurasy on train and validation set w.r.t. network complexity).\n",
        "3. Try to deal with overfitting (at least partially) by using regularization techniques (Dropout/Batchnorm/...) and demonstrate the results.\n",
        "\n",
        "__Please, write a small report describing your ideas, tries and achieved results in the end of this file.__\n",
        "\n",
        "*Note*: Tasks 2 and 3 are interrelated, in task 3 your goal is to make the network from task 2 less prone to overfitting. Task 1 is independent from 2 and 3.\n",
        "\n",
        "*Note 2*: We recomment to use Google Colab or other machine with GPU acceleration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KBld6VOSwhW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67c664b4-5131-4551-d9bf-8b6e887ac44d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchsummary\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "# device = 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdLOG0XqS_g5",
        "outputId": "029f52dd-2cb3-4091-98cc-52a10afe10c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory fmnist is created!\n"
          ]
        }
      ],
      "source": [
        "# Technical function\n",
        "def mkdir(path):\n",
        "    if not os.path.exists(root_path):\n",
        "        os.mkdir(root_path)\n",
        "        print('Directory', path, 'is created!')\n",
        "    else:\n",
        "        print('Directory', path, 'already exists!')\n",
        "\n",
        "root_path = 'fmnist'\n",
        "mkdir(root_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qt6LE7XaTDT9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b41a3580-5b9b-47f3-8840-fba715d0e5a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to fmnist/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 13605862.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting fmnist/FashionMNIST/raw/train-images-idx3-ubyte.gz to fmnist/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to fmnist/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 230211.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting fmnist/FashionMNIST/raw/train-labels-idx1-ubyte.gz to fmnist/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to fmnist/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 4245000.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting fmnist/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to fmnist/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to fmnist/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 19629342.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting fmnist/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to fmnist/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "download = True\n",
        "train_transform = transforms.ToTensor()\n",
        "test_transform = transforms.ToTensor()\n",
        "transforms.Compose((transforms.ToTensor()))\n",
        "\n",
        "\n",
        "fmnist_dataset_train = torchvision.datasets.FashionMNIST(root_path,\n",
        "                                                        train=True,\n",
        "                                                        transform=train_transform,\n",
        "                                                        target_transform=None,\n",
        "                                                        download=download)\n",
        "fmnist_dataset_test = torchvision.datasets.FashionMNIST(root_path,\n",
        "                                                       train=False,\n",
        "                                                       transform=test_transform,\n",
        "                                                       target_transform=None,\n",
        "                                                       download=download)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71YP0SPwTIxD"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(fmnist_dataset_train,\n",
        "                                           batch_size=128,\n",
        "                                           shuffle=True,\n",
        "                                           num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(fmnist_dataset_test,\n",
        "                                          batch_size=256,\n",
        "                                          shuffle=False,\n",
        "                                          num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_YFmF7NTWrQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5aae7ee-e5c9-49db-da01-a119ec27a4ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "len(fmnist_dataset_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_batch = next(iter(train_loader))\n",
        "_image, _label = random_batch[0][0], random_batch[1][0]\n",
        "plt.figure()\n",
        "plt.imshow(_image.reshape(28, 28))\n",
        "plt.title(f'Image label: {_label}')"
      ],
      "metadata": {
        "id": "ES0CbPj9TwZI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "9930c2bb-9aa4-402a-db60-0e8c58697a57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Image label: 9')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHca15bOTY4B",
        "outputId": "fbca4002-4261-44b5-9791-008d8d1c582f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 1, 28, 28])\n",
            "torch.Size([128])\n",
            "128\n"
          ]
        }
      ],
      "source": [
        "for img, label in train_loader:\n",
        "    img, label = img.to(device), label.to(device)\n",
        "    print(img.shape)\n",
        "#     print(img)\n",
        "    print(label.shape)\n",
        "    print(label.size(0))\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6OOOffHTfX5"
      },
      "source": [
        "### Task 1\n",
        "Train a network that achieves $\\geq 0.885$ test accuracy. It's fine to use only Linear (`nn.Linear`) layers and activations/dropout/batchnorm. Convolutional layers might be a great use, but we will meet them a bit later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftpkTjxlTcFx"
      },
      "outputs": [],
      "source": [
        "class TinyNeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_shape=28*28, num_classes=10, input_channels=1):\n",
        "        super(self.__class__, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Flatten(), # This layer converts image into a vector to use Linear layers afterwards\n",
        "            # Your network structure comes here\n",
        "            nn.Linear(input_shape, 500),\n",
        "            nn.BatchNorm1d(500),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.05),\n",
        "            nn.Linear(500, 250),\n",
        "            nn.BatchNorm1d(250),\n",
        "            nn.GLU(),\n",
        "            nn.Dropout(0.05),\n",
        "            nn.Linear(125, num_classes),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, inp):\n",
        "        out = self.model(inp)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZrIV7X8SjXV",
        "outputId": "fb39da55-0223-4b8d-9e68-8f210d5dfbcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "           Flatten-1                  [-1, 784]               0\n",
            "            Linear-2                  [-1, 500]         392,500\n",
            "       BatchNorm1d-3                  [-1, 500]           1,000\n",
            "              ReLU-4                  [-1, 500]               0\n",
            "           Dropout-5                  [-1, 500]               0\n",
            "            Linear-6                  [-1, 250]         125,250\n",
            "       BatchNorm1d-7                  [-1, 250]             500\n",
            "               GLU-8                  [-1, 125]               0\n",
            "           Dropout-9                  [-1, 125]               0\n",
            "           Linear-10                   [-1, 10]           1,260\n",
            "             ReLU-11                   [-1, 10]               0\n",
            "================================================================\n",
            "Total params: 520,510\n",
            "Trainable params: 520,510\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.03\n",
            "Params size (MB): 1.99\n",
            "Estimated Total Size (MB): 2.02\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "torchsummary.summary(TinyNeuralNetwork().to(device), (28*28,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "544PGKEnjPr5"
      },
      "source": [
        "Your experiments come here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3POFj90Ti-6"
      },
      "outputs": [],
      "source": [
        "# model = TinyNeuralNetwork().to(device)\n",
        "# opt = optim.SGD(model.parameters(), lr=1e-3, momentum=0.97)\n",
        "# loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "# Your experiments, training and validation loops here"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, trainloader, criterion, optimizer, epochs=5, device='cpu', printing=True):\n",
        "    data_len = len(trainloader)\n",
        "    p_len = data_len // 5\n",
        "    if (p_len == 0):\n",
        "        p_len = 1\n",
        "    for epoch in (range(epochs)):  # loop over the dataset multiple times\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            # print(inputs, labels)\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            # print(outputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print('ok')\n",
        "            # print statistics\n",
        "            if (printing):\n",
        "                running_loss += loss.item()\n",
        "                if i % p_len == p_len - 1:\n",
        "                    print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / p_len:.3f}')\n",
        "                    running_loss = 0.0\n",
        "\n",
        "    print('Finished Training')\n",
        "def test(net, test_loader, device='cpu', printing=True):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            # calculate outputs by running images through the network\n",
        "            outputs = net(inputs)\n",
        "            # the class with the highest energy is what we choose as prediction\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accur = correct / total\n",
        "    if (printing):\n",
        "        print(f'Accuracy of the network: {100 * correct / total} %')\n",
        "    return accur"
      ],
      "metadata": {
        "id": "0WYVwXYiUeL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TinyNeuralNetwork().to(device)\n",
        "opt = optim.SGD(model.parameters(), lr=1e-2, momentum=0.8, nesterov=True)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "train(model, train_loader, loss_func, opt, 15, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUnZlmZtUfvy",
        "outputId": "885e4962-01b6-4a01-c2eb-c27d106686bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,    93] loss: 0.868\n",
            "[1,   186] loss: 0.509\n",
            "[1,   279] loss: 0.451\n",
            "[1,   372] loss: 0.415\n",
            "[1,   465] loss: 0.388\n",
            "[2,    93] loss: 0.369\n",
            "[2,   186] loss: 0.347\n",
            "[2,   279] loss: 0.349\n",
            "[2,   372] loss: 0.354\n",
            "[2,   465] loss: 0.346\n",
            "[3,    93] loss: 0.322\n",
            "[3,   186] loss: 0.316\n",
            "[3,   279] loss: 0.315\n",
            "[3,   372] loss: 0.305\n",
            "[3,   465] loss: 0.303\n",
            "[4,    93] loss: 0.283\n",
            "[4,   186] loss: 0.292\n",
            "[4,   279] loss: 0.286\n",
            "[4,   372] loss: 0.281\n",
            "[4,   465] loss: 0.285\n",
            "[5,    93] loss: 0.250\n",
            "[5,   186] loss: 0.267\n",
            "[5,   279] loss: 0.270\n",
            "[5,   372] loss: 0.262\n",
            "[5,   465] loss: 0.261\n",
            "[6,    93] loss: 0.242\n",
            "[6,   186] loss: 0.235\n",
            "[6,   279] loss: 0.251\n",
            "[6,   372] loss: 0.244\n",
            "[6,   465] loss: 0.253\n",
            "[7,    93] loss: 0.216\n",
            "[7,   186] loss: 0.228\n",
            "[7,   279] loss: 0.235\n",
            "[7,   372] loss: 0.229\n",
            "[7,   465] loss: 0.227\n",
            "[8,    93] loss: 0.216\n",
            "[8,   186] loss: 0.210\n",
            "[8,   279] loss: 0.212\n",
            "[8,   372] loss: 0.217\n",
            "[8,   465] loss: 0.211\n",
            "[9,    93] loss: 0.197\n",
            "[9,   186] loss: 0.200\n",
            "[9,   279] loss: 0.203\n",
            "[9,   372] loss: 0.210\n",
            "[9,   465] loss: 0.212\n",
            "[10,    93] loss: 0.186\n",
            "[10,   186] loss: 0.179\n",
            "[10,   279] loss: 0.191\n",
            "[10,   372] loss: 0.191\n",
            "[10,   465] loss: 0.203\n",
            "[11,    93] loss: 0.178\n",
            "[11,   186] loss: 0.186\n",
            "[11,   279] loss: 0.180\n",
            "[11,   372] loss: 0.179\n",
            "[11,   465] loss: 0.187\n",
            "[12,    93] loss: 0.168\n",
            "[12,   186] loss: 0.164\n",
            "[12,   279] loss: 0.176\n",
            "[12,   372] loss: 0.172\n",
            "[12,   465] loss: 0.169\n",
            "[13,    93] loss: 0.165\n",
            "[13,   186] loss: 0.159\n",
            "[13,   279] loss: 0.166\n",
            "[13,   372] loss: 0.164\n",
            "[13,   465] loss: 0.166\n",
            "[14,    93] loss: 0.146\n",
            "[14,   186] loss: 0.160\n",
            "[14,   279] loss: 0.157\n",
            "[14,   372] loss: 0.151\n",
            "[14,   465] loss: 0.161\n",
            "[15,    93] loss: 0.137\n",
            "[15,   186] loss: 0.142\n",
            "[15,   279] loss: 0.156\n",
            "[15,   372] loss: 0.143\n",
            "[15,   465] loss: 0.160\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(model, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d9C2iZmC0z-",
        "outputId": "d99973fe-82fe-4783-dd23-d37b0562d177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network: 88.78 %\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8878"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7ISqkjmCPB1"
      },
      "source": [
        "### Task 2: Overfit it.\n",
        "Build a network that will overfit to this dataset. Demonstrate the overfitting in the appropriate way (e.g. plot loss and accurasy on train and test set w.r.t. network complexity).\n",
        "\n",
        "*Note:* you also might decrease the size of `train` dataset to enforce the overfitting and speed up the computations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H12uAWiGBwJx"
      },
      "outputs": [],
      "source": [
        "class OverfittingNeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_shape=28*28, num_classes=10, input_channels=1):\n",
        "        super(self.__class__, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(input_shape, 512),\n",
        "            nn.Linear(512, num_classes)\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, inp):\n",
        "        return self.model(inp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgXAKCpvCwqH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c60784a-b9cc-40b3-f0eb-a19c2ec7c3ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "           Flatten-1                  [-1, 784]               0\n",
            "            Linear-2                  [-1, 512]         401,920\n",
            "            Linear-3                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 407,050\n",
            "Trainable params: 407,050\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.01\n",
            "Params size (MB): 1.55\n",
            "Estimated Total Size (MB): 1.57\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "torchsummary.summary(OverfittingNeuralNetwork().to(device), (28*28,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bguBvfhNSjXY"
      },
      "outputs": [],
      "source": [
        "model_overfitt = OverfittingNeuralNetwork().to(device)\n",
        "opt = optim.SGD(model_overfitt.parameters(), lr=1e-3, momentum=0.9)\n",
        "loss_func = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import SubsetRandomSampler\n",
        "train_indices = list(range(10000))\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "train_overfitting_loader = torch.utils.data.DataLoader(fmnist_dataset_train,\n",
        "                                                       batch_size=128,\n",
        "                                                       sampler=train_sampler,\n",
        "                                                       num_workers=2)"
      ],
      "metadata": {
        "id": "oA2C2VePIK8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model_overfitt, train_overfitting_loader, loss_func, opt, 20, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U50_0VjUIF8U",
        "outputId": "75af92d5-bb16-4f14-db82-897f57e44ac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,    15] loss: 2.251\n",
            "[1,    30] loss: 2.109\n",
            "[1,    45] loss: 1.963\n",
            "[1,    60] loss: 1.848\n",
            "[1,    75] loss: 1.717\n",
            "[2,    15] loss: 1.577\n",
            "[2,    30] loss: 1.506\n",
            "[2,    45] loss: 1.421\n",
            "[2,    60] loss: 1.358\n",
            "[2,    75] loss: 1.284\n",
            "[3,    15] loss: 1.217\n",
            "[3,    30] loss: 1.212\n",
            "[3,    45] loss: 1.129\n",
            "[3,    60] loss: 1.134\n",
            "[3,    75] loss: 1.078\n",
            "[4,    15] loss: 1.028\n",
            "[4,    30] loss: 1.018\n",
            "[4,    45] loss: 1.025\n",
            "[4,    60] loss: 0.981\n",
            "[4,    75] loss: 0.985\n",
            "[5,    15] loss: 0.930\n",
            "[5,    30] loss: 0.916\n",
            "[5,    45] loss: 0.920\n",
            "[5,    60] loss: 0.917\n",
            "[5,    75] loss: 0.914\n",
            "[6,    15] loss: 0.869\n",
            "[6,    30] loss: 0.861\n",
            "[6,    45] loss: 0.859\n",
            "[6,    60] loss: 0.850\n",
            "[6,    75] loss: 0.876\n",
            "[7,    15] loss: 0.827\n",
            "[7,    30] loss: 0.832\n",
            "[7,    45] loss: 0.804\n",
            "[7,    60] loss: 0.812\n",
            "[7,    75] loss: 0.810\n",
            "[8,    15] loss: 0.810\n",
            "[8,    30] loss: 0.763\n",
            "[8,    45] loss: 0.772\n",
            "[8,    60] loss: 0.806\n",
            "[8,    75] loss: 0.779\n",
            "[9,    15] loss: 0.786\n",
            "[9,    30] loss: 0.754\n",
            "[9,    45] loss: 0.752\n",
            "[9,    60] loss: 0.764\n",
            "[9,    75] loss: 0.733\n",
            "[10,    15] loss: 0.752\n",
            "[10,    30] loss: 0.723\n",
            "[10,    45] loss: 0.732\n",
            "[10,    60] loss: 0.751\n",
            "[10,    75] loss: 0.713\n",
            "[11,    15] loss: 0.745\n",
            "[11,    30] loss: 0.711\n",
            "[11,    45] loss: 0.697\n",
            "[11,    60] loss: 0.677\n",
            "[11,    75] loss: 0.734\n",
            "[12,    15] loss: 0.709\n",
            "[12,    30] loss: 0.733\n",
            "[12,    45] loss: 0.677\n",
            "[12,    60] loss: 0.715\n",
            "[12,    75] loss: 0.652\n",
            "[13,    15] loss: 0.677\n",
            "[13,    30] loss: 0.681\n",
            "[13,    45] loss: 0.666\n",
            "[13,    60] loss: 0.676\n",
            "[13,    75] loss: 0.690\n",
            "[14,    15] loss: 0.680\n",
            "[14,    30] loss: 0.640\n",
            "[14,    45] loss: 0.685\n",
            "[14,    60] loss: 0.654\n",
            "[14,    75] loss: 0.664\n",
            "[15,    15] loss: 0.643\n",
            "[15,    30] loss: 0.680\n",
            "[15,    45] loss: 0.642\n",
            "[15,    60] loss: 0.633\n",
            "[15,    75] loss: 0.657\n",
            "[16,    15] loss: 0.619\n",
            "[16,    30] loss: 0.608\n",
            "[16,    45] loss: 0.650\n",
            "[16,    60] loss: 0.652\n",
            "[16,    75] loss: 0.677\n",
            "[17,    15] loss: 0.621\n",
            "[17,    30] loss: 0.625\n",
            "[17,    45] loss: 0.646\n",
            "[17,    60] loss: 0.634\n",
            "[17,    75] loss: 0.619\n",
            "[18,    15] loss: 0.607\n",
            "[18,    30] loss: 0.616\n",
            "[18,    45] loss: 0.619\n",
            "[18,    60] loss: 0.632\n",
            "[18,    75] loss: 0.605\n",
            "[19,    15] loss: 0.613\n",
            "[19,    30] loss: 0.607\n",
            "[19,    45] loss: 0.570\n",
            "[19,    60] loss: 0.611\n",
            "[19,    75] loss: 0.624\n",
            "[20,    15] loss: 0.580\n",
            "[20,    30] loss: 0.605\n",
            "[20,    45] loss: 0.602\n",
            "[20,    60] loss: 0.611\n",
            "[20,    75] loss: 0.597\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(model_overfitt, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG_VaKqEEoyA",
        "outputId": "638c2e79-2473-4ed1-dd19-7ff9f36428ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network: 78.24 %\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7824"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OverfittingNeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_shape=28*28, num_classes=10, input_channels=1, layers=2, neurons=100):\n",
        "        super(self.__class__, self).__init__()\n",
        "        # neurons = 512\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linears = nn.ModuleList([nn.Linear(input_shape, layers*neurons)])\n",
        "        for l in range(layers, 1, -1):\n",
        "            self.linears.append(nn.Linear(l * neurons, (l - 1) * neurons))\n",
        "        self.linears.append(nn.Linear(neurons, num_classes))\n",
        "    def forward(self, inp):\n",
        "        x = self.flatten(inp)\n",
        "        for linear_layer in self.linears:\n",
        "            x = linear_layer(x)\n",
        "            x = torch.relu(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "UmOKb450x0Wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, trainloader, criterion, optimizer, epochs=5, device='cpu', printing=True):\n",
        "    data_len = len(trainloader)\n",
        "    max_loss = 0.7\n",
        "    appl_epachs = 5\n",
        "    p_len = data_len // 5\n",
        "    if (p_len == 0):\n",
        "        p_len = 1\n",
        "    for epoch in (range(epochs)):  # loop over the dataset multiple times\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            # print(inputs, labels)\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            # print(outputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print('ok')\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            # if (epoch >= epochs-1 and running_loss > max_loss):\n",
        "            #     print(f'running_loss={running_loss} so big!!')\n",
        "            if (printing):\n",
        "                if i % p_len == p_len - 1:\n",
        "                    print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / p_len:.3f}')\n",
        "                    running_loss = 0.0"
      ],
      "metadata": {
        "id": "lRugR0Us7x2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "class OverfittingNeuralNetworkL(nn.Module):\n",
        "    def __init__(self, input_shape=28*28, num_classes=10, input_channels=1, layers=2, neurons=100):\n",
        "        super(OverfittingNeuralNetworkL, self).__init__()\n",
        "        # neurons = 100\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linears = nn.ModuleList([\n",
        "            nn.Linear(input_shape, layers * neurons)\n",
        "        ])\n",
        "        for l in range(layers, 1, -1):\n",
        "            self.linears.append(nn.Linear(l * neurons, (l - 1) * neurons))\n",
        "        self.output_layer = nn.Linear(neurons, num_classes)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        x = self.flatten(inp)\n",
        "        for linear_layer in self.linears:\n",
        "            x = linear_layer(x)\n",
        "            x = torch.relu(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "def train(net, trainloader, criterion, optimizer, epochs=5, device='cpu', printing=True):\n",
        "    data_len = len(trainloader)\n",
        "    max_loss = 0.7\n",
        "    appl_epochs = 5\n",
        "    p_len = data_len // 5\n",
        "    if p_len == 0:\n",
        "        p_len = 1\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 1):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            if printing and i % p_len == 0:\n",
        "                print(f'[{epoch}, {i:5d}] loss: {running_loss / p_len:.3f}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "\n",
        "layers = [2, 3, 4, 5, 6]\n",
        "neurons = [50, 100, 200, 350, 500]\n",
        "accurs = []\n",
        "\n",
        "for l in (layers):\n",
        "    for n in neurons:\n",
        "        print(f'l={l}; n={n}')\n",
        "        model_ov = OverfittingNeuralNetworkL(layers=l, neurons=n).to(device)\n",
        "        opt = optim.SGD(model_ov.parameters(), lr=1e-2, momentum=0.8)\n",
        "        loss_func = nn.CrossEntropyLoss()\n",
        "        train(model_ov, train_overfitting_loader, loss_func, opt, 20, device, printing=True)\n",
        "        accurs.append(test(model_ov, test_loader, device, printing=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58W_lBg9IzLX",
        "outputId": "08880e05-ce96-48c7-adb9-9944026db5f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "l=2; n=50\n",
            "[1,    15] loss: 2.290\n",
            "[1,    30] loss: 2.243\n",
            "[1,    45] loss: 2.167\n",
            "[1,    60] loss: 2.042\n",
            "[1,    75] loss: 1.852\n",
            "[2,    15] loss: 1.567\n",
            "[2,    30] loss: 1.360\n",
            "[2,    45] loss: 1.196\n",
            "[2,    60] loss: 1.110\n",
            "[2,    75] loss: 1.023\n",
            "[3,    15] loss: 0.961\n",
            "[3,    30] loss: 0.925\n",
            "[3,    45] loss: 0.886\n",
            "[3,    60] loss: 0.842\n",
            "[3,    75] loss: 0.835\n",
            "[4,    15] loss: 0.830\n",
            "[4,    30] loss: 0.766\n",
            "[4,    45] loss: 0.742\n",
            "[4,    60] loss: 0.735\n",
            "[4,    75] loss: 0.735\n",
            "[5,    15] loss: 0.740\n",
            "[5,    30] loss: 0.704\n",
            "[5,    45] loss: 0.722\n",
            "[5,    60] loss: 0.699\n",
            "[5,    75] loss: 0.667\n",
            "[6,    15] loss: 0.667\n",
            "[6,    30] loss: 0.679\n",
            "[6,    45] loss: 0.654\n",
            "[6,    60] loss: 0.661\n",
            "[6,    75] loss: 0.636\n",
            "[7,    15] loss: 0.625\n",
            "[7,    30] loss: 0.616\n",
            "[7,    45] loss: 0.606\n",
            "[7,    60] loss: 0.609\n",
            "[7,    75] loss: 0.589\n",
            "[8,    15] loss: 0.601\n",
            "[8,    30] loss: 0.626\n",
            "[8,    45] loss: 0.572\n",
            "[8,    60] loss: 0.563\n",
            "[8,    75] loss: 0.551\n",
            "[9,    15] loss: 0.563\n",
            "[9,    30] loss: 0.568\n",
            "[9,    45] loss: 0.555\n",
            "[9,    60] loss: 0.557\n",
            "[9,    75] loss: 0.526\n",
            "[10,    15] loss: 0.556\n",
            "[10,    30] loss: 0.537\n",
            "[10,    45] loss: 0.534\n",
            "[10,    60] loss: 0.518\n",
            "[10,    75] loss: 0.492\n",
            "[11,    15] loss: 0.535\n",
            "[11,    30] loss: 0.542\n",
            "[11,    45] loss: 0.506\n",
            "[11,    60] loss: 0.503\n",
            "[11,    75] loss: 0.516\n",
            "[12,    15] loss: 0.503\n",
            "[12,    30] loss: 0.499\n",
            "[12,    45] loss: 0.488\n",
            "[12,    60] loss: 0.491\n",
            "[12,    75] loss: 0.515\n",
            "[13,    15] loss: 0.484\n",
            "[13,    30] loss: 0.499\n",
            "[13,    45] loss: 0.472\n",
            "[13,    60] loss: 0.486\n",
            "[13,    75] loss: 0.511\n",
            "[14,    15] loss: 0.470\n",
            "[14,    30] loss: 0.450\n",
            "[14,    45] loss: 0.489\n",
            "[14,    60] loss: 0.474\n",
            "[14,    75] loss: 0.480\n",
            "[15,    15] loss: 0.465\n",
            "[15,    30] loss: 0.471\n",
            "[15,    45] loss: 0.443\n",
            "[15,    60] loss: 0.454\n",
            "[15,    75] loss: 0.474\n",
            "[16,    15] loss: 0.460\n",
            "[16,    30] loss: 0.461\n",
            "[16,    45] loss: 0.452\n",
            "[16,    60] loss: 0.421\n",
            "[16,    75] loss: 0.454\n",
            "[17,    15] loss: 0.486\n",
            "[17,    30] loss: 0.421\n",
            "[17,    45] loss: 0.432\n",
            "[17,    60] loss: 0.435\n",
            "[17,    75] loss: 0.468\n",
            "[18,    15] loss: 0.459\n",
            "[18,    30] loss: 0.424\n",
            "[18,    45] loss: 0.467\n",
            "[18,    60] loss: 0.438\n",
            "[18,    75] loss: 0.452\n",
            "[19,    15] loss: 0.424\n",
            "[19,    30] loss: 0.462\n",
            "[19,    45] loss: 0.406\n",
            "[19,    60] loss: 0.435\n",
            "[19,    75] loss: 0.421\n",
            "[20,    15] loss: 0.424\n",
            "[20,    30] loss: 0.436\n",
            "[20,    45] loss: 0.405\n",
            "[20,    60] loss: 0.411\n",
            "[20,    75] loss: 0.439\n",
            "Accuracy of the network: 81.62 %\n",
            "l=2; n=100\n",
            "[1,    15] loss: 2.286\n",
            "[1,    30] loss: 2.211\n",
            "[1,    45] loss: 2.092\n",
            "[1,    60] loss: 1.886\n",
            "[1,    75] loss: 1.626\n",
            "[2,    15] loss: 1.350\n",
            "[2,    30] loss: 1.190\n",
            "[2,    45] loss: 1.106\n",
            "[2,    60] loss: 0.992\n",
            "[2,    75] loss: 0.963\n",
            "[3,    15] loss: 0.893\n",
            "[3,    30] loss: 0.877\n",
            "[3,    45] loss: 0.804\n",
            "[3,    60] loss: 0.820\n",
            "[3,    75] loss: 0.792\n",
            "[4,    15] loss: 0.773\n",
            "[4,    30] loss: 0.747\n",
            "[4,    45] loss: 0.726\n",
            "[4,    60] loss: 0.739\n",
            "[4,    75] loss: 0.727\n",
            "[5,    15] loss: 0.699\n",
            "[5,    30] loss: 0.706\n",
            "[5,    45] loss: 0.673\n",
            "[5,    60] loss: 0.674\n",
            "[5,    75] loss: 0.628\n",
            "[6,    15] loss: 0.633\n",
            "[6,    30] loss: 0.659\n",
            "[6,    45] loss: 0.646\n",
            "[6,    60] loss: 0.592\n",
            "[6,    75] loss: 0.595\n",
            "[7,    15] loss: 0.631\n",
            "[7,    30] loss: 0.604\n",
            "[7,    45] loss: 0.583\n",
            "[7,    60] loss: 0.560\n",
            "[7,    75] loss: 0.571\n",
            "[8,    15] loss: 0.576\n",
            "[8,    30] loss: 0.574\n",
            "[8,    45] loss: 0.579\n",
            "[8,    60] loss: 0.555\n",
            "[8,    75] loss: 0.524\n",
            "[9,    15] loss: 0.536\n",
            "[9,    30] loss: 0.532\n",
            "[9,    45] loss: 0.546\n",
            "[9,    60] loss: 0.527\n",
            "[9,    75] loss: 0.518\n",
            "[10,    15] loss: 0.546\n",
            "[10,    30] loss: 0.551\n",
            "[10,    45] loss: 0.477\n",
            "[10,    60] loss: 0.516\n",
            "[10,    75] loss: 0.496\n",
            "[11,    15] loss: 0.504\n",
            "[11,    30] loss: 0.520\n",
            "[11,    45] loss: 0.489\n",
            "[11,    60] loss: 0.495\n",
            "[11,    75] loss: 0.486\n",
            "[12,    15] loss: 0.475\n",
            "[12,    30] loss: 0.469\n",
            "[12,    45] loss: 0.498\n",
            "[12,    60] loss: 0.507\n",
            "[12,    75] loss: 0.468\n",
            "[13,    15] loss: 0.497\n",
            "[13,    30] loss: 0.435\n",
            "[13,    45] loss: 0.470\n",
            "[13,    60] loss: 0.471\n",
            "[13,    75] loss: 0.503\n",
            "[14,    15] loss: 0.468\n",
            "[14,    30] loss: 0.456\n",
            "[14,    45] loss: 0.479\n",
            "[14,    60] loss: 0.474\n",
            "[14,    75] loss: 0.437\n",
            "[15,    15] loss: 0.485\n",
            "[15,    30] loss: 0.435\n",
            "[15,    45] loss: 0.474\n",
            "[15,    60] loss: 0.427\n",
            "[15,    75] loss: 0.469\n",
            "[16,    15] loss: 0.422\n",
            "[16,    30] loss: 0.454\n",
            "[16,    45] loss: 0.447\n",
            "[16,    60] loss: 0.448\n",
            "[16,    75] loss: 0.445\n",
            "[17,    15] loss: 0.439\n",
            "[17,    30] loss: 0.419\n",
            "[17,    45] loss: 0.439\n",
            "[17,    60] loss: 0.466\n",
            "[17,    75] loss: 0.415\n",
            "[18,    15] loss: 0.463\n",
            "[18,    30] loss: 0.456\n",
            "[18,    45] loss: 0.431\n",
            "[18,    60] loss: 0.385\n",
            "[18,    75] loss: 0.405\n",
            "[19,    15] loss: 0.442\n",
            "[19,    30] loss: 0.439\n",
            "[19,    45] loss: 0.426\n",
            "[19,    60] loss: 0.421\n",
            "[19,    75] loss: 0.402\n",
            "[20,    15] loss: 0.443\n",
            "[20,    30] loss: 0.416\n",
            "[20,    45] loss: 0.422\n",
            "[20,    60] loss: 0.393\n",
            "[20,    75] loss: 0.408\n",
            "Accuracy of the network: 82.68 %\n",
            "l=2; n=200\n",
            "[1,    15] loss: 2.278\n",
            "[1,    30] loss: 2.190\n",
            "[1,    45] loss: 2.056\n",
            "[1,    60] loss: 1.854\n",
            "[1,    75] loss: 1.593\n",
            "[2,    15] loss: 1.318\n",
            "[2,    30] loss: 1.176\n",
            "[2,    45] loss: 1.046\n",
            "[2,    60] loss: 0.992\n",
            "[2,    75] loss: 0.935\n",
            "[3,    15] loss: 0.881\n",
            "[3,    30] loss: 0.832\n",
            "[3,    45] loss: 0.797\n",
            "[3,    60] loss: 0.809\n",
            "[3,    75] loss: 0.783\n",
            "[4,    15] loss: 0.730\n",
            "[4,    30] loss: 0.721\n",
            "[4,    45] loss: 0.725\n",
            "[4,    60] loss: 0.728\n",
            "[4,    75] loss: 0.695\n",
            "[5,    15] loss: 0.673\n",
            "[5,    30] loss: 0.652\n",
            "[5,    45] loss: 0.667\n",
            "[5,    60] loss: 0.654\n",
            "[5,    75] loss: 0.645\n",
            "[6,    15] loss: 0.636\n",
            "[6,    30] loss: 0.626\n",
            "[6,    45] loss: 0.622\n",
            "[6,    60] loss: 0.595\n",
            "[6,    75] loss: 0.589\n",
            "[7,    15] loss: 0.593\n",
            "[7,    30] loss: 0.597\n",
            "[7,    45] loss: 0.558\n",
            "[7,    60] loss: 0.574\n",
            "[7,    75] loss: 0.561\n",
            "[8,    15] loss: 0.537\n",
            "[8,    30] loss: 0.564\n",
            "[8,    45] loss: 0.544\n",
            "[8,    60] loss: 0.550\n",
            "[8,    75] loss: 0.519\n",
            "[9,    15] loss: 0.535\n",
            "[9,    30] loss: 0.517\n",
            "[9,    45] loss: 0.545\n",
            "[9,    60] loss: 0.497\n",
            "[9,    75] loss: 0.493\n",
            "[10,    15] loss: 0.520\n",
            "[10,    30] loss: 0.506\n",
            "[10,    45] loss: 0.496\n",
            "[10,    60] loss: 0.511\n",
            "[10,    75] loss: 0.468\n",
            "[11,    15] loss: 0.500\n",
            "[11,    30] loss: 0.516\n",
            "[11,    45] loss: 0.477\n",
            "[11,    60] loss: 0.516\n",
            "[11,    75] loss: 0.469\n",
            "[12,    15] loss: 0.464\n",
            "[12,    30] loss: 0.484\n",
            "[12,    45] loss: 0.480\n",
            "[12,    60] loss: 0.496\n",
            "[12,    75] loss: 0.437\n",
            "[13,    15] loss: 0.471\n",
            "[13,    30] loss: 0.484\n",
            "[13,    45] loss: 0.455\n",
            "[13,    60] loss: 0.472\n",
            "[13,    75] loss: 0.416\n",
            "[14,    15] loss: 0.441\n",
            "[14,    30] loss: 0.502\n",
            "[14,    45] loss: 0.422\n",
            "[14,    60] loss: 0.415\n",
            "[14,    75] loss: 0.481\n",
            "[15,    15] loss: 0.463\n",
            "[15,    30] loss: 0.471\n",
            "[15,    45] loss: 0.439\n",
            "[15,    60] loss: 0.416\n",
            "[15,    75] loss: 0.436\n",
            "[16,    15] loss: 0.461\n",
            "[16,    30] loss: 0.413\n",
            "[16,    45] loss: 0.410\n",
            "[16,    60] loss: 0.433\n",
            "[16,    75] loss: 0.470\n",
            "[17,    15] loss: 0.447\n",
            "[17,    30] loss: 0.445\n",
            "[17,    45] loss: 0.431\n",
            "[17,    60] loss: 0.422\n",
            "[17,    75] loss: 0.401\n",
            "[18,    15] loss: 0.471\n",
            "[18,    30] loss: 0.413\n",
            "[18,    45] loss: 0.404\n",
            "[18,    60] loss: 0.449\n",
            "[18,    75] loss: 0.413\n",
            "[19,    15] loss: 0.431\n",
            "[19,    30] loss: 0.419\n",
            "[19,    45] loss: 0.418\n",
            "[19,    60] loss: 0.418\n",
            "[19,    75] loss: 0.407\n",
            "[20,    15] loss: 0.393\n",
            "[20,    30] loss: 0.416\n",
            "[20,    45] loss: 0.399\n",
            "[20,    60] loss: 0.406\n",
            "[20,    75] loss: 0.423\n",
            "Accuracy of the network: 82.57 %\n",
            "l=2; n=350\n",
            "[1,    15] loss: 2.275\n",
            "[1,    30] loss: 2.181\n",
            "[1,    45] loss: 2.022\n",
            "[1,    60] loss: 1.792\n",
            "[1,    75] loss: 1.503\n",
            "[2,    15] loss: 1.240\n",
            "[2,    30] loss: 1.117\n",
            "[2,    45] loss: 0.994\n",
            "[2,    60] loss: 0.925\n",
            "[2,    75] loss: 0.888\n",
            "[3,    15] loss: 0.815\n",
            "[3,    30] loss: 0.819\n",
            "[3,    45] loss: 0.783\n",
            "[3,    60] loss: 0.804\n",
            "[3,    75] loss: 0.712\n",
            "[4,    15] loss: 0.665\n",
            "[4,    30] loss: 0.726\n",
            "[4,    45] loss: 0.712\n",
            "[4,    60] loss: 0.673\n",
            "[4,    75] loss: 0.708\n",
            "[5,    15] loss: 0.623\n",
            "[5,    30] loss: 0.633\n",
            "[5,    45] loss: 0.623\n",
            "[5,    60] loss: 0.635\n",
            "[5,    75] loss: 0.634\n",
            "[6,    15] loss: 0.601\n",
            "[6,    30] loss: 0.597\n",
            "[6,    45] loss: 0.597\n",
            "[6,    60] loss: 0.571\n",
            "[6,    75] loss: 0.613\n",
            "[7,    15] loss: 0.589\n",
            "[7,    30] loss: 0.557\n",
            "[7,    45] loss: 0.581\n",
            "[7,    60] loss: 0.525\n",
            "[7,    75] loss: 0.538\n",
            "[8,    15] loss: 0.541\n",
            "[8,    30] loss: 0.544\n",
            "[8,    45] loss: 0.522\n",
            "[8,    60] loss: 0.527\n",
            "[8,    75] loss: 0.515\n",
            "[9,    15] loss: 0.526\n",
            "[9,    30] loss: 0.505\n",
            "[9,    45] loss: 0.507\n",
            "[9,    60] loss: 0.511\n",
            "[9,    75] loss: 0.486\n",
            "[10,    15] loss: 0.511\n",
            "[10,    30] loss: 0.513\n",
            "[10,    45] loss: 0.502\n",
            "[10,    60] loss: 0.497\n",
            "[10,    75] loss: 0.484\n",
            "[11,    15] loss: 0.508\n",
            "[11,    30] loss: 0.465\n",
            "[11,    45] loss: 0.500\n",
            "[11,    60] loss: 0.461\n",
            "[11,    75] loss: 0.482\n",
            "[12,    15] loss: 0.473\n",
            "[12,    30] loss: 0.481\n",
            "[12,    45] loss: 0.450\n",
            "[12,    60] loss: 0.462\n",
            "[12,    75] loss: 0.481\n",
            "[13,    15] loss: 0.459\n",
            "[13,    30] loss: 0.475\n",
            "[13,    45] loss: 0.457\n",
            "[13,    60] loss: 0.443\n",
            "[13,    75] loss: 0.453\n",
            "[14,    15] loss: 0.443\n",
            "[14,    30] loss: 0.454\n",
            "[14,    45] loss: 0.453\n",
            "[14,    60] loss: 0.445\n",
            "[14,    75] loss: 0.432\n",
            "[15,    15] loss: 0.455\n",
            "[15,    30] loss: 0.435\n",
            "[15,    45] loss: 0.435\n",
            "[15,    60] loss: 0.446\n",
            "[15,    75] loss: 0.425\n",
            "[16,    15] loss: 0.450\n",
            "[16,    30] loss: 0.395\n",
            "[16,    45] loss: 0.446\n",
            "[16,    60] loss: 0.456\n",
            "[16,    75] loss: 0.452\n",
            "[17,    15] loss: 0.406\n",
            "[17,    30] loss: 0.421\n",
            "[17,    45] loss: 0.416\n",
            "[17,    60] loss: 0.429\n",
            "[17,    75] loss: 0.424\n",
            "[18,    15] loss: 0.423\n",
            "[18,    30] loss: 0.424\n",
            "[18,    45] loss: 0.394\n",
            "[18,    60] loss: 0.433\n",
            "[18,    75] loss: 0.390\n",
            "[19,    15] loss: 0.426\n",
            "[19,    30] loss: 0.435\n",
            "[19,    45] loss: 0.404\n",
            "[19,    60] loss: 0.403\n",
            "[19,    75] loss: 0.373\n",
            "[20,    15] loss: 0.419\n",
            "[20,    30] loss: 0.435\n",
            "[20,    45] loss: 0.365\n",
            "[20,    60] loss: 0.410\n",
            "[20,    75] loss: 0.390\n",
            "Accuracy of the network: 79.85 %\n",
            "l=2; n=500\n",
            "[1,    15] loss: 2.267\n",
            "[1,    30] loss: 2.144\n",
            "[1,    45] loss: 1.956\n",
            "[1,    60] loss: 1.669\n",
            "[1,    75] loss: 1.403\n",
            "[2,    15] loss: 1.175\n",
            "[2,    30] loss: 1.063\n",
            "[2,    45] loss: 0.968\n",
            "[2,    60] loss: 0.888\n",
            "[2,    75] loss: 0.870\n",
            "[3,    15] loss: 0.801\n",
            "[3,    30] loss: 0.773\n",
            "[3,    45] loss: 0.769\n",
            "[3,    60] loss: 0.789\n",
            "[3,    75] loss: 0.739\n",
            "[4,    15] loss: 0.707\n",
            "[4,    30] loss: 0.721\n",
            "[4,    45] loss: 0.685\n",
            "[4,    60] loss: 0.669\n",
            "[4,    75] loss: 0.663\n",
            "[5,    15] loss: 0.677\n",
            "[5,    30] loss: 0.626\n",
            "[5,    45] loss: 0.642\n",
            "[5,    60] loss: 0.583\n",
            "[5,    75] loss: 0.613\n",
            "[6,    15] loss: 0.612\n",
            "[6,    30] loss: 0.574\n",
            "[6,    45] loss: 0.578\n",
            "[6,    60] loss: 0.550\n",
            "[6,    75] loss: 0.595\n",
            "[7,    15] loss: 0.587\n",
            "[7,    30] loss: 0.527\n",
            "[7,    45] loss: 0.558\n",
            "[7,    60] loss: 0.567\n",
            "[7,    75] loss: 0.517\n",
            "[8,    15] loss: 0.536\n",
            "[8,    30] loss: 0.533\n",
            "[8,    45] loss: 0.510\n",
            "[8,    60] loss: 0.513\n",
            "[8,    75] loss: 0.556\n",
            "[9,    15] loss: 0.576\n",
            "[9,    30] loss: 0.489\n",
            "[9,    45] loss: 0.494\n",
            "[9,    60] loss: 0.493\n",
            "[9,    75] loss: 0.503\n",
            "[10,    15] loss: 0.481\n",
            "[10,    30] loss: 0.489\n",
            "[10,    45] loss: 0.482\n",
            "[10,    60] loss: 0.456\n",
            "[10,    75] loss: 0.489\n",
            "[11,    15] loss: 0.496\n",
            "[11,    30] loss: 0.472\n",
            "[11,    45] loss: 0.458\n",
            "[11,    60] loss: 0.446\n",
            "[11,    75] loss: 0.483\n",
            "[12,    15] loss: 0.480\n",
            "[12,    30] loss: 0.490\n",
            "[12,    45] loss: 0.458\n",
            "[12,    60] loss: 0.466\n",
            "[12,    75] loss: 0.460\n",
            "[13,    15] loss: 0.479\n",
            "[13,    30] loss: 0.460\n",
            "[13,    45] loss: 0.437\n",
            "[13,    60] loss: 0.453\n",
            "[13,    75] loss: 0.429\n",
            "[14,    15] loss: 0.445\n",
            "[14,    30] loss: 0.421\n",
            "[14,    45] loss: 0.437\n",
            "[14,    60] loss: 0.462\n",
            "[14,    75] loss: 0.432\n",
            "[15,    15] loss: 0.427\n",
            "[15,    30] loss: 0.422\n",
            "[15,    45] loss: 0.424\n",
            "[15,    60] loss: 0.440\n",
            "[15,    75] loss: 0.435\n",
            "[16,    15] loss: 0.412\n",
            "[16,    30] loss: 0.407\n",
            "[16,    45] loss: 0.431\n",
            "[16,    60] loss: 0.405\n",
            "[16,    75] loss: 0.434\n",
            "[17,    15] loss: 0.412\n",
            "[17,    30] loss: 0.392\n",
            "[17,    45] loss: 0.406\n",
            "[17,    60] loss: 0.440\n",
            "[17,    75] loss: 0.428\n",
            "[18,    15] loss: 0.417\n",
            "[18,    30] loss: 0.388\n",
            "[18,    45] loss: 0.399\n",
            "[18,    60] loss: 0.409\n",
            "[18,    75] loss: 0.396\n",
            "[19,    15] loss: 0.439\n",
            "[19,    30] loss: 0.411\n",
            "[19,    45] loss: 0.407\n",
            "[19,    60] loss: 0.413\n",
            "[19,    75] loss: 0.366\n",
            "[20,    15] loss: 0.421\n",
            "[20,    30] loss: 0.362\n",
            "[20,    45] loss: 0.372\n",
            "[20,    60] loss: 0.395\n",
            "[20,    75] loss: 0.404\n",
            "Accuracy of the network: 83.39 %\n",
            "l=3; n=50\n",
            "[1,    15] loss: 2.303\n",
            "[1,    30] loss: 2.292\n",
            "[1,    45] loss: 2.283\n",
            "[1,    60] loss: 2.271\n",
            "[1,    75] loss: 2.245\n",
            "[2,    15] loss: 2.195\n",
            "[2,    30] loss: 2.114\n",
            "[2,    45] loss: 1.963\n",
            "[2,    60] loss: 1.747\n",
            "[2,    75] loss: 1.529\n",
            "[3,    15] loss: 1.306\n",
            "[3,    30] loss: 1.216\n",
            "[3,    45] loss: 1.116\n",
            "[3,    60] loss: 1.027\n",
            "[3,    75] loss: 1.012\n",
            "[4,    15] loss: 0.925\n",
            "[4,    30] loss: 0.893\n",
            "[4,    45] loss: 0.866\n",
            "[4,    60] loss: 0.862\n",
            "[4,    75] loss: 0.863\n",
            "[5,    15] loss: 0.799\n",
            "[5,    30] loss: 0.792\n",
            "[5,    45] loss: 0.845\n",
            "[5,    60] loss: 0.760\n",
            "[5,    75] loss: 0.748\n",
            "[6,    15] loss: 0.726\n",
            "[6,    30] loss: 0.717\n",
            "[6,    45] loss: 0.708\n",
            "[6,    60] loss: 0.701\n",
            "[6,    75] loss: 0.697\n",
            "[7,    15] loss: 0.678\n",
            "[7,    30] loss: 0.685\n",
            "[7,    45] loss: 0.657\n",
            "[7,    60] loss: 0.631\n",
            "[7,    75] loss: 0.659\n",
            "[8,    15] loss: 0.651\n",
            "[8,    30] loss: 0.645\n",
            "[8,    45] loss: 0.614\n",
            "[8,    60] loss: 0.631\n",
            "[8,    75] loss: 0.596\n",
            "[9,    15] loss: 0.604\n",
            "[9,    30] loss: 0.599\n",
            "[9,    45] loss: 0.602\n",
            "[9,    60] loss: 0.585\n",
            "[9,    75] loss: 0.593\n",
            "[10,    15] loss: 0.652\n",
            "[10,    30] loss: 0.573\n",
            "[10,    45] loss: 0.574\n",
            "[10,    60] loss: 0.571\n",
            "[10,    75] loss: 0.570\n",
            "[11,    15] loss: 0.562\n",
            "[11,    30] loss: 0.544\n",
            "[11,    45] loss: 0.569\n",
            "[11,    60] loss: 0.536\n",
            "[11,    75] loss: 0.569\n",
            "[12,    15] loss: 0.589\n",
            "[12,    30] loss: 0.514\n",
            "[12,    45] loss: 0.554\n",
            "[12,    60] loss: 0.554\n",
            "[12,    75] loss: 0.506\n",
            "[13,    15] loss: 0.542\n",
            "[13,    30] loss: 0.521\n",
            "[13,    45] loss: 0.508\n",
            "[13,    60] loss: 0.532\n",
            "[13,    75] loss: 0.512\n",
            "[14,    15] loss: 0.517\n",
            "[14,    30] loss: 0.498\n",
            "[14,    45] loss: 0.486\n",
            "[14,    60] loss: 0.544\n",
            "[14,    75] loss: 0.524\n",
            "[15,    15] loss: 0.527\n",
            "[15,    30] loss: 0.518\n",
            "[15,    45] loss: 0.471\n",
            "[15,    60] loss: 0.488\n",
            "[15,    75] loss: 0.492\n",
            "[16,    15] loss: 0.483\n",
            "[16,    30] loss: 0.456\n",
            "[16,    45] loss: 0.497\n",
            "[16,    60] loss: 0.509\n",
            "[16,    75] loss: 0.487\n",
            "[17,    15] loss: 0.481\n",
            "[17,    30] loss: 0.472\n",
            "[17,    45] loss: 0.463\n",
            "[17,    60] loss: 0.452\n",
            "[17,    75] loss: 0.493\n",
            "[18,    15] loss: 0.451\n",
            "[18,    30] loss: 0.505\n",
            "[18,    45] loss: 0.442\n",
            "[18,    60] loss: 0.469\n",
            "[18,    75] loss: 0.460\n",
            "[19,    15] loss: 0.470\n",
            "[19,    30] loss: 0.443\n",
            "[19,    45] loss: 0.456\n",
            "[19,    60] loss: 0.475\n",
            "[19,    75] loss: 0.444\n",
            "[20,    15] loss: 0.421\n",
            "[20,    30] loss: 0.462\n",
            "[20,    45] loss: 0.464\n",
            "[20,    60] loss: 0.475\n",
            "[20,    75] loss: 0.414\n",
            "Accuracy of the network: 80.22 %\n",
            "l=3; n=100\n",
            "[1,    15] loss: 2.302\n",
            "[1,    30] loss: 2.290\n",
            "[1,    45] loss: 2.274\n",
            "[1,    60] loss: 2.250\n",
            "[1,    75] loss: 2.226\n",
            "[2,    15] loss: 2.150\n",
            "[2,    30] loss: 2.036\n",
            "[2,    45] loss: 1.841\n",
            "[2,    60] loss: 1.607\n",
            "[2,    75] loss: 1.376\n",
            "[3,    15] loss: 1.215\n",
            "[3,    30] loss: 1.133\n",
            "[3,    45] loss: 1.053\n",
            "[3,    60] loss: 0.993\n",
            "[3,    75] loss: 0.967\n",
            "[4,    15] loss: 0.911\n",
            "[4,    30] loss: 0.890\n",
            "[4,    45] loss: 0.861\n",
            "[4,    60] loss: 0.860\n",
            "[4,    75] loss: 0.812\n",
            "[5,    15] loss: 0.835\n",
            "[5,    30] loss: 0.792\n",
            "[5,    45] loss: 0.757\n",
            "[5,    60] loss: 0.741\n",
            "[5,    75] loss: 0.759\n",
            "[6,    15] loss: 0.720\n",
            "[6,    30] loss: 0.695\n",
            "[6,    45] loss: 0.714\n",
            "[6,    60] loss: 0.718\n",
            "[6,    75] loss: 0.693\n",
            "[7,    15] loss: 0.692\n",
            "[7,    30] loss: 0.622\n",
            "[7,    45] loss: 0.662\n",
            "[7,    60] loss: 0.620\n",
            "[7,    75] loss: 0.671\n",
            "[8,    15] loss: 0.627\n",
            "[8,    30] loss: 0.620\n",
            "[8,    45] loss: 0.636\n",
            "[8,    60] loss: 0.573\n",
            "[8,    75] loss: 0.577\n",
            "[9,    15] loss: 0.620\n",
            "[9,    30] loss: 0.623\n",
            "[9,    45] loss: 0.609\n",
            "[9,    60] loss: 0.576\n",
            "[9,    75] loss: 0.548\n",
            "[10,    15] loss: 0.545\n",
            "[10,    30] loss: 0.567\n",
            "[10,    45] loss: 0.543\n",
            "[10,    60] loss: 0.578\n",
            "[10,    75] loss: 0.561\n",
            "[11,    15] loss: 0.558\n",
            "[11,    30] loss: 0.543\n",
            "[11,    45] loss: 0.490\n",
            "[11,    60] loss: 0.533\n",
            "[11,    75] loss: 0.553\n",
            "[12,    15] loss: 0.512\n",
            "[12,    30] loss: 0.501\n",
            "[12,    45] loss: 0.509\n",
            "[12,    60] loss: 0.512\n",
            "[12,    75] loss: 0.518\n",
            "[13,    15] loss: 0.503\n",
            "[13,    30] loss: 0.465\n",
            "[13,    45] loss: 0.488\n",
            "[13,    60] loss: 0.520\n",
            "[13,    75] loss: 0.508\n",
            "[14,    15] loss: 0.509\n",
            "[14,    30] loss: 0.517\n",
            "[14,    45] loss: 0.487\n",
            "[14,    60] loss: 0.477\n",
            "[14,    75] loss: 0.472\n",
            "[15,    15] loss: 0.486\n",
            "[15,    30] loss: 0.491\n",
            "[15,    45] loss: 0.472\n",
            "[15,    60] loss: 0.466\n",
            "[15,    75] loss: 0.464\n",
            "[16,    15] loss: 0.494\n",
            "[16,    30] loss: 0.487\n",
            "[16,    45] loss: 0.458\n",
            "[16,    60] loss: 0.417\n",
            "[16,    75] loss: 0.447\n",
            "[17,    15] loss: 0.478\n",
            "[17,    30] loss: 0.429\n",
            "[17,    45] loss: 0.431\n",
            "[17,    60] loss: 0.441\n",
            "[17,    75] loss: 0.426\n",
            "[18,    15] loss: 0.443\n",
            "[18,    30] loss: 0.405\n",
            "[18,    45] loss: 0.447\n",
            "[18,    60] loss: 0.462\n",
            "[18,    75] loss: 0.426\n",
            "[19,    15] loss: 0.482\n",
            "[19,    30] loss: 0.446\n",
            "[19,    45] loss: 0.391\n",
            "[19,    60] loss: 0.436\n",
            "[19,    75] loss: 0.406\n",
            "[20,    15] loss: 0.436\n",
            "[20,    30] loss: 0.436\n",
            "[20,    45] loss: 0.418\n",
            "[20,    60] loss: 0.404\n",
            "[20,    75] loss: 0.407\n",
            "Accuracy of the network: 82.65 %\n",
            "l=3; n=200\n",
            "[1,    15] loss: 2.299\n",
            "[1,    30] loss: 2.279\n",
            "[1,    45] loss: 2.257\n",
            "[1,    60] loss: 2.216\n",
            "[1,    75] loss: 2.155\n",
            "[2,    15] loss: 1.987\n",
            "[2,    30] loss: 1.768\n",
            "[2,    45] loss: 1.536\n",
            "[2,    60] loss: 1.367\n",
            "[2,    75] loss: 1.239\n",
            "[3,    15] loss: 1.143\n",
            "[3,    30] loss: 1.031\n",
            "[3,    45] loss: 0.997\n",
            "[3,    60] loss: 0.966\n",
            "[3,    75] loss: 0.884\n",
            "[4,    15] loss: 0.870\n",
            "[4,    30] loss: 0.861\n",
            "[4,    45] loss: 0.811\n",
            "[4,    60] loss: 0.805\n",
            "[4,    75] loss: 0.789\n",
            "[5,    15] loss: 0.769\n",
            "[5,    30] loss: 0.734\n",
            "[5,    45] loss: 0.733\n",
            "[5,    60] loss: 0.737\n",
            "[5,    75] loss: 0.746\n",
            "[6,    15] loss: 0.713\n",
            "[6,    30] loss: 0.721\n",
            "[6,    45] loss: 0.726\n",
            "[6,    60] loss: 0.659\n",
            "[6,    75] loss: 0.662\n",
            "[7,    15] loss: 0.706\n",
            "[7,    30] loss: 0.655\n",
            "[7,    45] loss: 0.659\n",
            "[7,    60] loss: 0.619\n",
            "[7,    75] loss: 0.616\n",
            "[8,    15] loss: 0.639\n",
            "[8,    30] loss: 0.598\n",
            "[8,    45] loss: 0.565\n",
            "[8,    60] loss: 0.594\n",
            "[8,    75] loss: 0.598\n",
            "[9,    15] loss: 0.595\n",
            "[9,    30] loss: 0.561\n",
            "[9,    45] loss: 0.564\n",
            "[9,    60] loss: 0.570\n",
            "[9,    75] loss: 0.552\n",
            "[10,    15] loss: 0.540\n",
            "[10,    30] loss: 0.579\n",
            "[10,    45] loss: 0.542\n",
            "[10,    60] loss: 0.522\n",
            "[10,    75] loss: 0.540\n",
            "[11,    15] loss: 0.540\n",
            "[11,    30] loss: 0.497\n",
            "[11,    45] loss: 0.509\n",
            "[11,    60] loss: 0.521\n",
            "[11,    75] loss: 0.519\n",
            "[12,    15] loss: 0.508\n",
            "[12,    30] loss: 0.487\n",
            "[12,    45] loss: 0.492\n",
            "[12,    60] loss: 0.528\n",
            "[12,    75] loss: 0.540\n",
            "[13,    15] loss: 0.489\n",
            "[13,    30] loss: 0.492\n",
            "[13,    45] loss: 0.507\n",
            "[13,    60] loss: 0.473\n",
            "[13,    75] loss: 0.501\n",
            "[14,    15] loss: 0.528\n",
            "[14,    30] loss: 0.469\n",
            "[14,    45] loss: 0.475\n",
            "[14,    60] loss: 0.460\n",
            "[14,    75] loss: 0.446\n",
            "[15,    15] loss: 0.467\n",
            "[15,    30] loss: 0.446\n",
            "[15,    45] loss: 0.432\n",
            "[15,    60] loss: 0.438\n",
            "[15,    75] loss: 0.498\n",
            "[16,    15] loss: 0.455\n",
            "[16,    30] loss: 0.419\n",
            "[16,    45] loss: 0.476\n",
            "[16,    60] loss: 0.435\n",
            "[16,    75] loss: 0.452\n",
            "[17,    15] loss: 0.477\n",
            "[17,    30] loss: 0.422\n",
            "[17,    45] loss: 0.429\n",
            "[17,    60] loss: 0.424\n",
            "[17,    75] loss: 0.426\n",
            "[18,    15] loss: 0.432\n",
            "[18,    30] loss: 0.422\n",
            "[18,    45] loss: 0.440\n",
            "[18,    60] loss: 0.424\n",
            "[18,    75] loss: 0.429\n",
            "[19,    15] loss: 0.423\n",
            "[19,    30] loss: 0.400\n",
            "[19,    45] loss: 0.409\n",
            "[19,    60] loss: 0.422\n",
            "[19,    75] loss: 0.419\n",
            "[20,    15] loss: 0.418\n",
            "[20,    30] loss: 0.402\n",
            "[20,    45] loss: 0.402\n",
            "[20,    60] loss: 0.387\n",
            "[20,    75] loss: 0.431\n",
            "Accuracy of the network: 81.77 %\n",
            "l=3; n=350\n",
            "[1,    15] loss: 2.297\n",
            "[1,    30] loss: 2.271\n",
            "[1,    45] loss: 2.232\n",
            "[1,    60] loss: 2.176\n",
            "[1,    75] loss: 2.061\n",
            "[2,    15] loss: 1.820\n",
            "[2,    30] loss: 1.572\n",
            "[2,    45] loss: 1.347\n",
            "[2,    60] loss: 1.211\n",
            "[2,    75] loss: 1.130\n",
            "[3,    15] loss: 1.053\n",
            "[3,    30] loss: 0.985\n",
            "[3,    45] loss: 0.921\n",
            "[3,    60] loss: 0.890\n",
            "[3,    75] loss: 0.838\n",
            "[4,    15] loss: 0.858\n",
            "[4,    30] loss: 0.798\n",
            "[4,    45] loss: 0.822\n",
            "[4,    60] loss: 0.757\n",
            "[4,    75] loss: 0.726\n",
            "[5,    15] loss: 0.743\n",
            "[5,    30] loss: 0.731\n",
            "[5,    45] loss: 0.733\n",
            "[5,    60] loss: 0.686\n",
            "[5,    75] loss: 0.662\n",
            "[6,    15] loss: 0.717\n",
            "[6,    30] loss: 0.666\n",
            "[6,    45] loss: 0.620\n",
            "[6,    60] loss: 0.613\n",
            "[6,    75] loss: 0.635\n",
            "[7,    15] loss: 0.628\n",
            "[7,    30] loss: 0.614\n",
            "[7,    45] loss: 0.610\n",
            "[7,    60] loss: 0.597\n",
            "[7,    75] loss: 0.561\n",
            "[8,    15] loss: 0.558\n",
            "[8,    30] loss: 0.610\n",
            "[8,    45] loss: 0.552\n",
            "[8,    60] loss: 0.572\n",
            "[8,    75] loss: 0.567\n",
            "[9,    15] loss: 0.569\n",
            "[9,    30] loss: 0.535\n",
            "[9,    45] loss: 0.539\n",
            "[9,    60] loss: 0.560\n",
            "[9,    75] loss: 0.518\n",
            "[10,    15] loss: 0.526\n",
            "[10,    30] loss: 0.512\n",
            "[10,    45] loss: 0.540\n",
            "[10,    60] loss: 0.536\n",
            "[10,    75] loss: 0.525\n",
            "[11,    15] loss: 0.511\n",
            "[11,    30] loss: 0.482\n",
            "[11,    45] loss: 0.523\n",
            "[11,    60] loss: 0.521\n",
            "[11,    75] loss: 0.520\n",
            "[12,    15] loss: 0.468\n",
            "[12,    30] loss: 0.474\n",
            "[12,    45] loss: 0.479\n",
            "[12,    60] loss: 0.481\n",
            "[12,    75] loss: 0.486\n",
            "[13,    15] loss: 0.453\n",
            "[13,    30] loss: 0.451\n",
            "[13,    45] loss: 0.496\n",
            "[13,    60] loss: 0.475\n",
            "[13,    75] loss: 0.454\n",
            "[14,    15] loss: 0.459\n",
            "[14,    30] loss: 0.457\n",
            "[14,    45] loss: 0.445\n",
            "[14,    60] loss: 0.459\n",
            "[14,    75] loss: 0.438\n",
            "[15,    15] loss: 0.415\n",
            "[15,    30] loss: 0.508\n",
            "[15,    45] loss: 0.477\n",
            "[15,    60] loss: 0.414\n",
            "[15,    75] loss: 0.408\n",
            "[16,    15] loss: 0.447\n",
            "[16,    30] loss: 0.433\n",
            "[16,    45] loss: 0.395\n",
            "[16,    60] loss: 0.439\n",
            "[16,    75] loss: 0.439\n",
            "[17,    15] loss: 0.434\n",
            "[17,    30] loss: 0.423\n",
            "[17,    45] loss: 0.386\n",
            "[17,    60] loss: 0.409\n",
            "[17,    75] loss: 0.411\n",
            "[18,    15] loss: 0.431\n",
            "[18,    30] loss: 0.405\n",
            "[18,    45] loss: 0.399\n",
            "[18,    60] loss: 0.401\n",
            "[18,    75] loss: 0.400\n",
            "[19,    15] loss: 0.394\n",
            "[19,    30] loss: 0.425\n",
            "[19,    45] loss: 0.385\n",
            "[19,    60] loss: 0.394\n",
            "[19,    75] loss: 0.389\n",
            "[20,    15] loss: 0.384\n",
            "[20,    30] loss: 0.393\n",
            "[20,    45] loss: 0.388\n",
            "[20,    60] loss: 0.387\n",
            "[20,    75] loss: 0.392\n",
            "Accuracy of the network: 82.6 %\n",
            "l=3; n=500\n",
            "[1,    15] loss: 2.293\n",
            "[1,    30] loss: 2.258\n",
            "[1,    45] loss: 2.212\n",
            "[1,    60] loss: 2.122\n",
            "[1,    75] loss: 1.969\n",
            "[2,    15] loss: 1.654\n",
            "[2,    30] loss: 1.399\n",
            "[2,    45] loss: 1.244\n",
            "[2,    60] loss: 1.138\n",
            "[2,    75] loss: 1.072\n",
            "[3,    15] loss: 0.971\n",
            "[3,    30] loss: 0.951\n",
            "[3,    45] loss: 0.897\n",
            "[3,    60] loss: 0.826\n",
            "[3,    75] loss: 0.832\n",
            "[4,    15] loss: 0.776\n",
            "[4,    30] loss: 0.758\n",
            "[4,    45] loss: 0.767\n",
            "[4,    60] loss: 0.764\n",
            "[4,    75] loss: 0.753\n",
            "[5,    15] loss: 0.750\n",
            "[5,    30] loss: 0.713\n",
            "[5,    45] loss: 0.705\n",
            "[5,    60] loss: 0.649\n",
            "[5,    75] loss: 0.698\n",
            "[6,    15] loss: 0.678\n",
            "[6,    30] loss: 0.640\n",
            "[6,    45] loss: 0.627\n",
            "[6,    60] loss: 0.638\n",
            "[6,    75] loss: 0.615\n",
            "[7,    15] loss: 0.603\n",
            "[7,    30] loss: 0.590\n",
            "[7,    45] loss: 0.622\n",
            "[7,    60] loss: 0.558\n",
            "[7,    75] loss: 0.610\n",
            "[8,    15] loss: 0.578\n",
            "[8,    30] loss: 0.546\n",
            "[8,    45] loss: 0.576\n",
            "[8,    60] loss: 0.527\n",
            "[8,    75] loss: 0.550\n",
            "[9,    15] loss: 0.526\n",
            "[9,    30] loss: 0.485\n",
            "[9,    45] loss: 0.554\n",
            "[9,    60] loss: 0.507\n",
            "[9,    75] loss: 0.543\n",
            "[10,    15] loss: 0.524\n",
            "[10,    30] loss: 0.482\n",
            "[10,    45] loss: 0.508\n",
            "[10,    60] loss: 0.477\n",
            "[10,    75] loss: 0.535\n",
            "[11,    15] loss: 0.497\n",
            "[11,    30] loss: 0.493\n",
            "[11,    45] loss: 0.475\n",
            "[11,    60] loss: 0.441\n",
            "[11,    75] loss: 0.493\n",
            "[12,    15] loss: 0.467\n",
            "[12,    30] loss: 0.501\n",
            "[12,    45] loss: 0.463\n",
            "[12,    60] loss: 0.456\n",
            "[12,    75] loss: 0.458\n",
            "[13,    15] loss: 0.431\n",
            "[13,    30] loss: 0.458\n",
            "[13,    45] loss: 0.448\n",
            "[13,    60] loss: 0.459\n",
            "[13,    75] loss: 0.463\n",
            "[14,    15] loss: 0.434\n",
            "[14,    30] loss: 0.457\n",
            "[14,    45] loss: 0.459\n",
            "[14,    60] loss: 0.425\n",
            "[14,    75] loss: 0.440\n",
            "[15,    15] loss: 0.407\n",
            "[15,    30] loss: 0.417\n",
            "[15,    45] loss: 0.438\n",
            "[15,    60] loss: 0.456\n",
            "[15,    75] loss: 0.430\n",
            "[16,    15] loss: 0.451\n",
            "[16,    30] loss: 0.410\n",
            "[16,    45] loss: 0.425\n",
            "[16,    60] loss: 0.432\n",
            "[16,    75] loss: 0.406\n",
            "[17,    15] loss: 0.423\n",
            "[17,    30] loss: 0.410\n",
            "[17,    45] loss: 0.423\n",
            "[17,    60] loss: 0.414\n",
            "[17,    75] loss: 0.396\n",
            "[18,    15] loss: 0.463\n",
            "[18,    30] loss: 0.403\n",
            "[18,    45] loss: 0.412\n",
            "[18,    60] loss: 0.387\n",
            "[18,    75] loss: 0.392\n",
            "[19,    15] loss: 0.409\n",
            "[19,    30] loss: 0.389\n",
            "[19,    45] loss: 0.403\n",
            "[19,    60] loss: 0.402\n",
            "[19,    75] loss: 0.379\n",
            "[20,    15] loss: 0.381\n",
            "[20,    30] loss: 0.354\n",
            "[20,    45] loss: 0.391\n",
            "[20,    60] loss: 0.413\n",
            "[20,    75] loss: 0.378\n",
            "Accuracy of the network: 82.18 %\n",
            "l=4; n=50\n",
            "[1,    15] loss: 2.309\n",
            "[1,    30] loss: 2.307\n",
            "[1,    45] loss: 2.303\n",
            "[1,    60] loss: 2.300\n",
            "[1,    75] loss: 2.299\n",
            "[2,    15] loss: 2.295\n",
            "[2,    30] loss: 2.293\n",
            "[2,    45] loss: 2.289\n",
            "[2,    60] loss: 2.284\n",
            "[2,    75] loss: 2.279\n",
            "[3,    15] loss: 2.269\n",
            "[3,    30] loss: 2.258\n",
            "[3,    45] loss: 2.243\n",
            "[3,    60] loss: 2.221\n",
            "[3,    75] loss: 2.187\n",
            "[4,    15] loss: 2.125\n",
            "[4,    30] loss: 2.040\n",
            "[4,    45] loss: 1.942\n",
            "[4,    60] loss: 1.800\n",
            "[4,    75] loss: 1.606\n",
            "[5,    15] loss: 1.333\n",
            "[5,    30] loss: 1.187\n",
            "[5,    45] loss: 1.075\n",
            "[5,    60] loss: 1.051\n",
            "[5,    75] loss: 1.021\n",
            "[6,    15] loss: 1.016\n",
            "[6,    30] loss: 0.963\n",
            "[6,    45] loss: 0.936\n",
            "[6,    60] loss: 0.905\n",
            "[6,    75] loss: 0.887\n",
            "[7,    15] loss: 0.871\n",
            "[7,    30] loss: 0.891\n",
            "[7,    45] loss: 0.886\n",
            "[7,    60] loss: 0.885\n",
            "[7,    75] loss: 0.878\n",
            "[8,    15] loss: 0.939\n",
            "[8,    30] loss: 0.824\n",
            "[8,    45] loss: 0.824\n",
            "[8,    60] loss: 0.818\n",
            "[8,    75] loss: 0.798\n",
            "[9,    15] loss: 0.816\n",
            "[9,    30] loss: 0.810\n",
            "[9,    45] loss: 0.741\n",
            "[9,    60] loss: 0.739\n",
            "[9,    75] loss: 0.763\n",
            "[10,    15] loss: 0.748\n",
            "[10,    30] loss: 0.751\n",
            "[10,    45] loss: 0.692\n",
            "[10,    60] loss: 0.693\n",
            "[10,    75] loss: 0.707\n",
            "[11,    15] loss: 0.736\n",
            "[11,    30] loss: 0.666\n",
            "[11,    45] loss: 0.640\n",
            "[11,    60] loss: 0.640\n",
            "[11,    75] loss: 0.682\n",
            "[12,    15] loss: 0.660\n",
            "[12,    30] loss: 0.616\n",
            "[12,    45] loss: 0.621\n",
            "[12,    60] loss: 0.653\n",
            "[12,    75] loss: 0.605\n",
            "[13,    15] loss: 0.605\n",
            "[13,    30] loss: 0.592\n",
            "[13,    45] loss: 0.591\n",
            "[13,    60] loss: 0.576\n",
            "[13,    75] loss: 0.628\n",
            "[14,    15] loss: 0.582\n",
            "[14,    30] loss: 0.545\n",
            "[14,    45] loss: 0.585\n",
            "[14,    60] loss: 0.574\n",
            "[14,    75] loss: 0.546\n",
            "[15,    15] loss: 0.575\n",
            "[15,    30] loss: 0.521\n",
            "[15,    45] loss: 0.571\n",
            "[15,    60] loss: 0.517\n",
            "[15,    75] loss: 0.567\n",
            "[16,    15] loss: 0.585\n",
            "[16,    30] loss: 0.519\n",
            "[16,    45] loss: 0.530\n",
            "[16,    60] loss: 0.529\n",
            "[16,    75] loss: 0.497\n",
            "[17,    15] loss: 0.539\n",
            "[17,    30] loss: 0.504\n",
            "[17,    45] loss: 0.492\n",
            "[17,    60] loss: 0.472\n",
            "[17,    75] loss: 0.484\n",
            "[18,    15] loss: 0.467\n",
            "[18,    30] loss: 0.493\n",
            "[18,    45] loss: 0.514\n",
            "[18,    60] loss: 0.505\n",
            "[18,    75] loss: 0.488\n",
            "[19,    15] loss: 0.501\n",
            "[19,    30] loss: 0.461\n",
            "[19,    45] loss: 0.451\n",
            "[19,    60] loss: 0.507\n",
            "[19,    75] loss: 0.468\n",
            "[20,    15] loss: 0.467\n",
            "[20,    30] loss: 0.439\n",
            "[20,    45] loss: 0.469\n",
            "[20,    60] loss: 0.449\n",
            "[20,    75] loss: 0.482\n",
            "Accuracy of the network: 78.77 %\n",
            "l=4; n=100\n",
            "[1,    15] loss: 2.303\n",
            "[1,    30] loss: 2.301\n",
            "[1,    45] loss: 2.297\n",
            "[1,    60] loss: 2.293\n",
            "[1,    75] loss: 2.290\n",
            "[2,    15] loss: 2.280\n",
            "[2,    30] loss: 2.274\n",
            "[2,    45] loss: 2.260\n",
            "[2,    60] loss: 2.243\n",
            "[2,    75] loss: 2.210\n",
            "[3,    15] loss: 2.130\n",
            "[3,    30] loss: 1.995\n",
            "[3,    45] loss: 1.794\n",
            "[3,    60] loss: 1.572\n",
            "[3,    75] loss: 1.410\n",
            "[4,    15] loss: 1.278\n",
            "[4,    30] loss: 1.227\n",
            "[4,    45] loss: 1.161\n",
            "[4,    60] loss: 1.110\n",
            "[4,    75] loss: 1.067\n",
            "[5,    15] loss: 1.072\n",
            "[5,    30] loss: 1.005\n",
            "[5,    45] loss: 1.007\n",
            "[5,    60] loss: 0.907\n",
            "[5,    75] loss: 0.889\n",
            "[6,    15] loss: 0.939\n",
            "[6,    30] loss: 0.876\n",
            "[6,    45] loss: 0.899\n",
            "[6,    60] loss: 0.858\n",
            "[6,    75] loss: 0.830\n",
            "[7,    15] loss: 0.884\n",
            "[7,    30] loss: 0.822\n",
            "[7,    45] loss: 0.788\n",
            "[7,    60] loss: 0.787\n",
            "[7,    75] loss: 0.821\n",
            "[8,    15] loss: 0.783\n",
            "[8,    30] loss: 0.797\n",
            "[8,    45] loss: 0.740\n",
            "[8,    60] loss: 0.743\n",
            "[8,    75] loss: 0.725\n",
            "[9,    15] loss: 0.716\n",
            "[9,    30] loss: 0.704\n",
            "[9,    45] loss: 0.723\n",
            "[9,    60] loss: 0.714\n",
            "[9,    75] loss: 0.676\n",
            "[10,    15] loss: 0.673\n",
            "[10,    30] loss: 0.633\n",
            "[10,    45] loss: 0.660\n",
            "[10,    60] loss: 0.613\n",
            "[10,    75] loss: 0.636\n",
            "[11,    15] loss: 0.583\n",
            "[11,    30] loss: 0.619\n",
            "[11,    45] loss: 0.606\n",
            "[11,    60] loss: 0.579\n",
            "[11,    75] loss: 0.593\n",
            "[12,    15] loss: 0.587\n",
            "[12,    30] loss: 0.549\n",
            "[12,    45] loss: 0.593\n",
            "[12,    60] loss: 0.569\n",
            "[12,    75] loss: 0.609\n",
            "[13,    15] loss: 0.559\n",
            "[13,    30] loss: 0.644\n",
            "[13,    45] loss: 0.539\n",
            "[13,    60] loss: 0.553\n",
            "[13,    75] loss: 0.540\n",
            "[14,    15] loss: 0.560\n",
            "[14,    30] loss: 0.522\n",
            "[14,    45] loss: 0.535\n",
            "[14,    60] loss: 0.531\n",
            "[14,    75] loss: 0.532\n",
            "[15,    15] loss: 0.526\n",
            "[15,    30] loss: 0.519\n",
            "[15,    45] loss: 0.496\n",
            "[15,    60] loss: 0.518\n",
            "[15,    75] loss: 0.529\n",
            "[16,    15] loss: 0.551\n",
            "[16,    30] loss: 0.506\n",
            "[16,    45] loss: 0.507\n",
            "[16,    60] loss: 0.576\n",
            "[16,    75] loss: 0.488\n",
            "[17,    15] loss: 0.486\n",
            "[17,    30] loss: 0.501\n",
            "[17,    45] loss: 0.507\n",
            "[17,    60] loss: 0.488\n",
            "[17,    75] loss: 0.517\n",
            "[18,    15] loss: 0.465\n",
            "[18,    30] loss: 0.508\n",
            "[18,    45] loss: 0.498\n",
            "[18,    60] loss: 0.481\n",
            "[18,    75] loss: 0.473\n",
            "[19,    15] loss: 0.471\n",
            "[19,    30] loss: 0.451\n",
            "[19,    45] loss: 0.453\n",
            "[19,    60] loss: 0.449\n",
            "[19,    75] loss: 0.468\n",
            "[20,    15] loss: 0.539\n",
            "[20,    30] loss: 0.426\n",
            "[20,    45] loss: 0.424\n",
            "[20,    60] loss: 0.472\n",
            "[20,    75] loss: 0.466\n",
            "Accuracy of the network: 75.68 %\n",
            "l=4; n=200\n",
            "[1,    15] loss: 2.303\n",
            "[1,    30] loss: 2.299\n",
            "[1,    45] loss: 2.292\n",
            "[1,    60] loss: 2.285\n",
            "[1,    75] loss: 2.277\n",
            "[2,    15] loss: 2.263\n",
            "[2,    30] loss: 2.247\n",
            "[2,    45] loss: 2.222\n",
            "[2,    60] loss: 2.172\n",
            "[2,    75] loss: 2.093\n",
            "[3,    15] loss: 1.894\n",
            "[3,    30] loss: 1.693\n",
            "[3,    45] loss: 1.475\n",
            "[3,    60] loss: 1.319\n",
            "[3,    75] loss: 1.193\n",
            "[4,    15] loss: 1.145\n",
            "[4,    30] loss: 1.099\n",
            "[4,    45] loss: 1.069\n",
            "[4,    60] loss: 1.016\n",
            "[4,    75] loss: 0.991\n",
            "[5,    15] loss: 0.965\n",
            "[5,    30] loss: 0.914\n",
            "[5,    45] loss: 0.906\n",
            "[5,    60] loss: 0.874\n",
            "[5,    75] loss: 0.933\n",
            "[6,    15] loss: 0.813\n",
            "[6,    30] loss: 0.862\n",
            "[6,    45] loss: 0.840\n",
            "[6,    60] loss: 0.800\n",
            "[6,    75] loss: 0.776\n",
            "[7,    15] loss: 0.773\n",
            "[7,    30] loss: 0.731\n",
            "[7,    45] loss: 0.699\n",
            "[7,    60] loss: 0.729\n",
            "[7,    75] loss: 0.735\n",
            "[8,    15] loss: 0.717\n",
            "[8,    30] loss: 0.681\n",
            "[8,    45] loss: 0.682\n",
            "[8,    60] loss: 0.693\n",
            "[8,    75] loss: 0.635\n",
            "[9,    15] loss: 0.654\n",
            "[9,    30] loss: 0.643\n",
            "[9,    45] loss: 0.613\n",
            "[9,    60] loss: 0.594\n",
            "[9,    75] loss: 0.655\n",
            "[10,    15] loss: 0.599\n",
            "[10,    30] loss: 0.623\n",
            "[10,    45] loss: 0.622\n",
            "[10,    60] loss: 0.598\n",
            "[10,    75] loss: 0.587\n",
            "[11,    15] loss: 0.620\n",
            "[11,    30] loss: 0.599\n",
            "[11,    45] loss: 0.579\n",
            "[11,    60] loss: 0.574\n",
            "[11,    75] loss: 0.556\n",
            "[12,    15] loss: 0.599\n",
            "[12,    30] loss: 0.507\n",
            "[12,    45] loss: 0.561\n",
            "[12,    60] loss: 0.582\n",
            "[12,    75] loss: 0.557\n",
            "[13,    15] loss: 0.545\n",
            "[13,    30] loss: 0.528\n",
            "[13,    45] loss: 0.533\n",
            "[13,    60] loss: 0.551\n",
            "[13,    75] loss: 0.522\n",
            "[14,    15] loss: 0.542\n",
            "[14,    30] loss: 0.519\n",
            "[14,    45] loss: 0.538\n",
            "[14,    60] loss: 0.504\n",
            "[14,    75] loss: 0.504\n",
            "[15,    15] loss: 0.501\n",
            "[15,    30] loss: 0.489\n",
            "[15,    45] loss: 0.508\n",
            "[15,    60] loss: 0.555\n",
            "[15,    75] loss: 0.470\n",
            "[16,    15] loss: 0.503\n",
            "[16,    30] loss: 0.481\n",
            "[16,    45] loss: 0.488\n",
            "[16,    60] loss: 0.465\n",
            "[16,    75] loss: 0.489\n",
            "[17,    15] loss: 0.541\n",
            "[17,    30] loss: 0.483\n",
            "[17,    45] loss: 0.473\n",
            "[17,    60] loss: 0.482\n",
            "[17,    75] loss: 0.482\n",
            "[18,    15] loss: 0.437\n",
            "[18,    30] loss: 0.485\n",
            "[18,    45] loss: 0.479\n",
            "[18,    60] loss: 0.474\n",
            "[18,    75] loss: 0.452\n",
            "[19,    15] loss: 0.455\n",
            "[19,    30] loss: 0.427\n",
            "[19,    45] loss: 0.451\n",
            "[19,    60] loss: 0.468\n",
            "[19,    75] loss: 0.447\n",
            "[20,    15] loss: 0.410\n",
            "[20,    30] loss: 0.451\n",
            "[20,    45] loss: 0.406\n",
            "[20,    60] loss: 0.418\n",
            "[20,    75] loss: 0.455\n",
            "Accuracy of the network: 81.69 %\n",
            "l=4; n=350\n",
            "[1,    15] loss: 2.301\n",
            "[1,    30] loss: 2.293\n",
            "[1,    45] loss: 2.287\n",
            "[1,    60] loss: 2.275\n",
            "[1,    75] loss: 2.261\n",
            "[2,    15] loss: 2.230\n",
            "[2,    30] loss: 2.187\n",
            "[2,    45] loss: 2.116\n",
            "[2,    60] loss: 1.973\n",
            "[2,    75] loss: 1.756\n",
            "[3,    15] loss: 1.482\n",
            "[3,    30] loss: 1.306\n",
            "[3,    45] loss: 1.205\n",
            "[3,    60] loss: 1.140\n",
            "[3,    75] loss: 1.077\n",
            "[4,    15] loss: 1.033\n",
            "[4,    30] loss: 0.966\n",
            "[4,    45] loss: 0.956\n",
            "[4,    60] loss: 0.911\n",
            "[4,    75] loss: 0.862\n",
            "[5,    15] loss: 0.878\n",
            "[5,    30] loss: 0.839\n",
            "[5,    45] loss: 0.862\n",
            "[5,    60] loss: 0.806\n",
            "[5,    75] loss: 0.784\n",
            "[6,    15] loss: 0.823\n",
            "[6,    30] loss: 0.778\n",
            "[6,    45] loss: 0.762\n",
            "[6,    60] loss: 0.767\n",
            "[6,    75] loss: 0.728\n",
            "[7,    15] loss: 0.704\n",
            "[7,    30] loss: 0.703\n",
            "[7,    45] loss: 0.696\n",
            "[7,    60] loss: 0.690\n",
            "[7,    75] loss: 0.637\n",
            "[8,    15] loss: 0.673\n",
            "[8,    30] loss: 0.638\n",
            "[8,    45] loss: 0.678\n",
            "[8,    60] loss: 0.656\n",
            "[8,    75] loss: 0.631\n",
            "[9,    15] loss: 0.677\n",
            "[9,    30] loss: 0.599\n",
            "[9,    45] loss: 0.644\n",
            "[9,    60] loss: 0.609\n",
            "[9,    75] loss: 0.578\n",
            "[10,    15] loss: 0.567\n",
            "[10,    30] loss: 0.601\n",
            "[10,    45] loss: 0.569\n",
            "[10,    60] loss: 0.613\n",
            "[10,    75] loss: 0.603\n",
            "[11,    15] loss: 0.564\n",
            "[11,    30] loss: 0.570\n",
            "[11,    45] loss: 0.563\n",
            "[11,    60] loss: 0.585\n",
            "[11,    75] loss: 0.537\n",
            "[12,    15] loss: 0.520\n",
            "[12,    30] loss: 0.508\n",
            "[12,    45] loss: 0.542\n",
            "[12,    60] loss: 0.533\n",
            "[12,    75] loss: 0.538\n",
            "[13,    15] loss: 0.523\n",
            "[13,    30] loss: 0.509\n",
            "[13,    45] loss: 0.493\n",
            "[13,    60] loss: 0.500\n",
            "[13,    75] loss: 0.519\n",
            "[14,    15] loss: 0.522\n",
            "[14,    30] loss: 0.483\n",
            "[14,    45] loss: 0.485\n",
            "[14,    60] loss: 0.510\n",
            "[14,    75] loss: 0.503\n",
            "[15,    15] loss: 0.463\n",
            "[15,    30] loss: 0.447\n",
            "[15,    45] loss: 0.489\n",
            "[15,    60] loss: 0.522\n",
            "[15,    75] loss: 0.479\n",
            "[16,    15] loss: 0.426\n",
            "[16,    30] loss: 0.470\n",
            "[16,    45] loss: 0.471\n",
            "[16,    60] loss: 0.489\n",
            "[16,    75] loss: 0.445\n",
            "[17,    15] loss: 0.443\n",
            "[17,    30] loss: 0.460\n",
            "[17,    45] loss: 0.441\n",
            "[17,    60] loss: 0.462\n",
            "[17,    75] loss: 0.410\n",
            "[18,    15] loss: 0.430\n",
            "[18,    30] loss: 0.436\n",
            "[18,    45] loss: 0.461\n",
            "[18,    60] loss: 0.391\n",
            "[18,    75] loss: 0.451\n",
            "[19,    15] loss: 0.392\n",
            "[19,    30] loss: 0.403\n",
            "[19,    45] loss: 0.414\n",
            "[19,    60] loss: 0.435\n",
            "[19,    75] loss: 0.429\n",
            "[20,    15] loss: 0.412\n",
            "[20,    30] loss: 0.435\n",
            "[20,    45] loss: 0.415\n",
            "[20,    60] loss: 0.411\n",
            "[20,    75] loss: 0.373\n",
            "Accuracy of the network: 80.31 %\n",
            "l=4; n=500\n",
            "[1,    15] loss: 2.299\n",
            "[1,    30] loss: 2.291\n",
            "[1,    45] loss: 2.277\n",
            "[1,    60] loss: 2.262\n",
            "[1,    75] loss: 2.238\n",
            "[2,    15] loss: 2.184\n",
            "[2,    30] loss: 2.098\n",
            "[2,    45] loss: 1.947\n",
            "[2,    60] loss: 1.695\n",
            "[2,    75] loss: 1.439\n",
            "[3,    15] loss: 1.249\n",
            "[3,    30] loss: 1.148\n",
            "[3,    45] loss: 1.068\n",
            "[3,    60] loss: 0.999\n",
            "[3,    75] loss: 0.955\n",
            "[4,    15] loss: 0.915\n",
            "[4,    30] loss: 0.882\n",
            "[4,    45] loss: 0.851\n",
            "[4,    60] loss: 0.869\n",
            "[4,    75] loss: 0.839\n",
            "[5,    15] loss: 0.817\n",
            "[5,    30] loss: 0.762\n",
            "[5,    45] loss: 0.777\n",
            "[5,    60] loss: 0.807\n",
            "[5,    75] loss: 0.779\n",
            "[6,    15] loss: 0.749\n",
            "[6,    30] loss: 0.723\n",
            "[6,    45] loss: 0.735\n",
            "[6,    60] loss: 0.692\n",
            "[6,    75] loss: 0.688\n",
            "[7,    15] loss: 0.691\n",
            "[7,    30] loss: 0.660\n",
            "[7,    45] loss: 0.643\n",
            "[7,    60] loss: 0.679\n",
            "[7,    75] loss: 0.615\n",
            "[8,    15] loss: 0.618\n",
            "[8,    30] loss: 0.687\n",
            "[8,    45] loss: 0.598\n",
            "[8,    60] loss: 0.611\n",
            "[8,    75] loss: 0.555\n",
            "[9,    15] loss: 0.561\n",
            "[9,    30] loss: 0.580\n",
            "[9,    45] loss: 0.588\n",
            "[9,    60] loss: 0.565\n",
            "[9,    75] loss: 0.545\n",
            "[10,    15] loss: 0.541\n",
            "[10,    30] loss: 0.557\n",
            "[10,    45] loss: 0.543\n",
            "[10,    60] loss: 0.547\n",
            "[10,    75] loss: 0.525\n",
            "[11,    15] loss: 0.564\n",
            "[11,    30] loss: 0.532\n",
            "[11,    45] loss: 0.522\n",
            "[11,    60] loss: 0.533\n",
            "[11,    75] loss: 0.498\n",
            "[12,    15] loss: 0.489\n",
            "[12,    30] loss: 0.490\n",
            "[12,    45] loss: 0.487\n",
            "[12,    60] loss: 0.494\n",
            "[12,    75] loss: 0.517\n",
            "[13,    15] loss: 0.529\n",
            "[13,    30] loss: 0.499\n",
            "[13,    45] loss: 0.478\n",
            "[13,    60] loss: 0.475\n",
            "[13,    75] loss: 0.503\n",
            "[14,    15] loss: 0.497\n",
            "[14,    30] loss: 0.422\n",
            "[14,    45] loss: 0.451\n",
            "[14,    60] loss: 0.470\n",
            "[14,    75] loss: 0.477\n",
            "[15,    15] loss: 0.484\n",
            "[15,    30] loss: 0.449\n",
            "[15,    45] loss: 0.429\n",
            "[15,    60] loss: 0.406\n",
            "[15,    75] loss: 0.461\n",
            "[16,    15] loss: 0.457\n",
            "[16,    30] loss: 0.427\n",
            "[16,    45] loss: 0.456\n",
            "[16,    60] loss: 0.422\n",
            "[16,    75] loss: 0.443\n",
            "[17,    15] loss: 0.392\n",
            "[17,    30] loss: 0.422\n",
            "[17,    45] loss: 0.420\n",
            "[17,    60] loss: 0.429\n",
            "[17,    75] loss: 0.446\n",
            "[18,    15] loss: 0.408\n",
            "[18,    30] loss: 0.407\n",
            "[18,    45] loss: 0.408\n",
            "[18,    60] loss: 0.417\n",
            "[18,    75] loss: 0.370\n",
            "[19,    15] loss: 0.439\n",
            "[19,    30] loss: 0.408\n",
            "[19,    45] loss: 0.386\n",
            "[19,    60] loss: 0.373\n",
            "[19,    75] loss: 0.388\n",
            "[20,    15] loss: 0.396\n",
            "[20,    30] loss: 0.351\n",
            "[20,    45] loss: 0.390\n",
            "[20,    60] loss: 0.369\n",
            "[20,    75] loss: 0.370\n",
            "Accuracy of the network: 82.35 %\n",
            "l=5; n=50\n",
            "[1,    15] loss: 2.308\n",
            "[1,    30] loss: 2.305\n",
            "[1,    45] loss: 2.302\n",
            "[1,    60] loss: 2.305\n",
            "[1,    75] loss: 2.301\n",
            "[2,    15] loss: 2.302\n",
            "[2,    30] loss: 2.301\n",
            "[2,    45] loss: 2.301\n",
            "[2,    60] loss: 2.300\n",
            "[2,    75] loss: 2.299\n",
            "[3,    15] loss: 2.298\n",
            "[3,    30] loss: 2.297\n",
            "[3,    45] loss: 2.296\n",
            "[3,    60] loss: 2.295\n",
            "[3,    75] loss: 2.295\n",
            "[4,    15] loss: 2.291\n",
            "[4,    30] loss: 2.290\n",
            "[4,    45] loss: 2.288\n",
            "[4,    60] loss: 2.283\n",
            "[4,    75] loss: 2.276\n",
            "[5,    15] loss: 2.269\n",
            "[5,    30] loss: 2.256\n",
            "[5,    45] loss: 2.231\n",
            "[5,    60] loss: 2.199\n",
            "[5,    75] loss: 2.158\n",
            "[6,    15] loss: 2.081\n",
            "[6,    30] loss: 2.015\n",
            "[6,    45] loss: 1.952\n",
            "[6,    60] loss: 1.880\n",
            "[6,    75] loss: 1.796\n",
            "[7,    15] loss: 1.636\n",
            "[7,    30] loss: 1.524\n",
            "[7,    45] loss: 1.423\n",
            "[7,    60] loss: 1.317\n",
            "[7,    75] loss: 1.258\n",
            "[8,    15] loss: 1.201\n",
            "[8,    30] loss: 1.167\n",
            "[8,    45] loss: 1.090\n",
            "[8,    60] loss: 1.022\n",
            "[8,    75] loss: 1.032\n",
            "[9,    15] loss: 1.013\n",
            "[9,    30] loss: 0.940\n",
            "[9,    45] loss: 0.960\n",
            "[9,    60] loss: 0.887\n",
            "[9,    75] loss: 0.980\n",
            "[10,    15] loss: 0.894\n",
            "[10,    30] loss: 0.816\n",
            "[10,    45] loss: 0.849\n",
            "[10,    60] loss: 0.822\n",
            "[10,    75] loss: 0.795\n",
            "[11,    15] loss: 0.809\n",
            "[11,    30] loss: 0.807\n",
            "[11,    45] loss: 0.783\n",
            "[11,    60] loss: 0.743\n",
            "[11,    75] loss: 0.709\n",
            "[12,    15] loss: 0.771\n",
            "[12,    30] loss: 0.702\n",
            "[12,    45] loss: 0.768\n",
            "[12,    60] loss: 0.686\n",
            "[12,    75] loss: 0.716\n",
            "[13,    15] loss: 0.666\n",
            "[13,    30] loss: 0.699\n",
            "[13,    45] loss: 0.673\n",
            "[13,    60] loss: 0.678\n",
            "[13,    75] loss: 0.705\n",
            "[14,    15] loss: 0.678\n",
            "[14,    30] loss: 0.612\n",
            "[14,    45] loss: 0.672\n",
            "[14,    60] loss: 0.661\n",
            "[14,    75] loss: 0.680\n",
            "[15,    15] loss: 0.627\n",
            "[15,    30] loss: 0.631\n",
            "[15,    45] loss: 0.627\n",
            "[15,    60] loss: 0.619\n",
            "[15,    75] loss: 0.603\n",
            "[16,    15] loss: 0.659\n",
            "[16,    30] loss: 0.635\n",
            "[16,    45] loss: 0.560\n",
            "[16,    60] loss: 0.578\n",
            "[16,    75] loss: 0.563\n",
            "[17,    15] loss: 0.607\n",
            "[17,    30] loss: 0.603\n",
            "[17,    45] loss: 0.612\n",
            "[17,    60] loss: 0.513\n",
            "[17,    75] loss: 0.550\n",
            "[18,    15] loss: 0.558\n",
            "[18,    30] loss: 0.552\n",
            "[18,    45] loss: 0.502\n",
            "[18,    60] loss: 0.584\n",
            "[18,    75] loss: 0.523\n",
            "[19,    15] loss: 0.555\n",
            "[19,    30] loss: 0.510\n",
            "[19,    45] loss: 0.545\n",
            "[19,    60] loss: 0.524\n",
            "[19,    75] loss: 0.509\n",
            "[20,    15] loss: 0.565\n",
            "[20,    30] loss: 0.481\n",
            "[20,    45] loss: 0.494\n",
            "[20,    60] loss: 0.468\n",
            "[20,    75] loss: 0.532\n",
            "Accuracy of the network: 80.38 %\n",
            "l=5; n=100\n",
            "[1,    15] loss: 2.305\n",
            "[1,    30] loss: 2.303\n",
            "[1,    45] loss: 2.303\n",
            "[1,    60] loss: 2.301\n",
            "[1,    75] loss: 2.302\n",
            "[2,    15] loss: 2.300\n",
            "[2,    30] loss: 2.299\n",
            "[2,    45] loss: 2.298\n",
            "[2,    60] loss: 2.297\n",
            "[2,    75] loss: 2.296\n",
            "[3,    15] loss: 2.294\n",
            "[3,    30] loss: 2.293\n",
            "[3,    45] loss: 2.291\n",
            "[3,    60] loss: 2.289\n",
            "[3,    75] loss: 2.286\n",
            "[4,    15] loss: 2.281\n",
            "[4,    30] loss: 2.274\n",
            "[4,    45] loss: 2.267\n",
            "[4,    60] loss: 2.257\n",
            "[4,    75] loss: 2.239\n",
            "[5,    15] loss: 2.206\n",
            "[5,    30] loss: 2.157\n",
            "[5,    45] loss: 2.086\n",
            "[5,    60] loss: 1.987\n",
            "[5,    75] loss: 1.861\n",
            "[6,    15] loss: 1.645\n",
            "[6,    30] loss: 1.456\n",
            "[6,    45] loss: 1.310\n",
            "[6,    60] loss: 1.211\n",
            "[6,    75] loss: 1.172\n",
            "[7,    15] loss: 1.079\n",
            "[7,    30] loss: 1.070\n",
            "[7,    45] loss: 1.036\n",
            "[7,    60] loss: 0.950\n",
            "[7,    75] loss: 0.952\n",
            "[8,    15] loss: 0.930\n",
            "[8,    30] loss: 0.921\n",
            "[8,    45] loss: 0.908\n",
            "[8,    60] loss: 0.942\n",
            "[8,    75] loss: 0.823\n",
            "[9,    15] loss: 0.902\n",
            "[9,    30] loss: 0.847\n",
            "[9,    45] loss: 0.844\n",
            "[9,    60] loss: 0.816\n",
            "[9,    75] loss: 0.853\n",
            "[10,    15] loss: 0.789\n",
            "[10,    30] loss: 0.793\n",
            "[10,    45] loss: 0.831\n",
            "[10,    60] loss: 0.770\n",
            "[10,    75] loss: 0.768\n",
            "[11,    15] loss: 0.889\n",
            "[11,    30] loss: 0.721\n",
            "[11,    45] loss: 0.726\n",
            "[11,    60] loss: 0.745\n",
            "[11,    75] loss: 0.795\n",
            "[12,    15] loss: 0.759\n",
            "[12,    30] loss: 0.735\n",
            "[12,    45] loss: 0.715\n",
            "[12,    60] loss: 0.730\n",
            "[12,    75] loss: 0.681\n",
            "[13,    15] loss: 0.821\n",
            "[13,    30] loss: 0.653\n",
            "[13,    45] loss: 0.700\n",
            "[13,    60] loss: 0.683\n",
            "[13,    75] loss: 0.695\n",
            "[14,    15] loss: 0.683\n",
            "[14,    30] loss: 0.633\n",
            "[14,    45] loss: 0.688\n",
            "[14,    60] loss: 0.636\n",
            "[14,    75] loss: 0.629\n",
            "[15,    15] loss: 0.742\n",
            "[15,    30] loss: 0.645\n",
            "[15,    45] loss: 0.635\n",
            "[15,    60] loss: 0.674\n",
            "[15,    75] loss: 0.634\n",
            "[16,    15] loss: 0.592\n",
            "[16,    30] loss: 0.608\n",
            "[16,    45] loss: 0.633\n",
            "[16,    60] loss: 0.682\n",
            "[16,    75] loss: 0.609\n",
            "[17,    15] loss: 0.572\n",
            "[17,    30] loss: 0.565\n",
            "[17,    45] loss: 0.611\n",
            "[17,    60] loss: 0.661\n",
            "[17,    75] loss: 0.580\n",
            "[18,    15] loss: 0.592\n",
            "[18,    30] loss: 0.573\n",
            "[18,    45] loss: 0.544\n",
            "[18,    60] loss: 0.551\n",
            "[18,    75] loss: 0.530\n",
            "[19,    15] loss: 0.571\n",
            "[19,    30] loss: 0.533\n",
            "[19,    45] loss: 0.536\n",
            "[19,    60] loss: 0.521\n",
            "[19,    75] loss: 0.526\n",
            "[20,    15] loss: 0.518\n",
            "[20,    30] loss: 0.510\n",
            "[20,    45] loss: 0.517\n",
            "[20,    60] loss: 0.534\n",
            "[20,    75] loss: 0.545\n",
            "Accuracy of the network: 74.35 %\n",
            "l=5; n=200\n",
            "[1,    15] loss: 2.302\n",
            "[1,    30] loss: 2.302\n",
            "[1,    45] loss: 2.302\n",
            "[1,    60] loss: 2.300\n",
            "[1,    75] loss: 2.298\n",
            "[2,    15] loss: 2.298\n",
            "[2,    30] loss: 2.295\n",
            "[2,    45] loss: 2.294\n",
            "[2,    60] loss: 2.292\n",
            "[2,    75] loss: 2.290\n",
            "[3,    15] loss: 2.286\n",
            "[3,    30] loss: 2.281\n",
            "[3,    45] loss: 2.275\n",
            "[3,    60] loss: 2.269\n",
            "[3,    75] loss: 2.258\n",
            "[4,    15] loss: 2.237\n",
            "[4,    30] loss: 2.211\n",
            "[4,    45] loss: 2.161\n",
            "[4,    60] loss: 2.071\n",
            "[4,    75] loss: 1.915\n",
            "[5,    15] loss: 1.636\n",
            "[5,    30] loss: 1.440\n",
            "[5,    45] loss: 1.314\n",
            "[5,    60] loss: 1.214\n",
            "[5,    75] loss: 1.156\n",
            "[6,    15] loss: 1.100\n",
            "[6,    30] loss: 1.077\n",
            "[6,    45] loss: 1.049\n",
            "[6,    60] loss: 1.015\n",
            "[6,    75] loss: 0.997\n",
            "[7,    15] loss: 1.011\n",
            "[7,    30] loss: 0.915\n",
            "[7,    45] loss: 0.907\n",
            "[7,    60] loss: 0.870\n",
            "[7,    75] loss: 0.900\n",
            "[8,    15] loss: 0.978\n",
            "[8,    30] loss: 0.853\n",
            "[8,    45] loss: 0.840\n",
            "[8,    60] loss: 0.856\n",
            "[8,    75] loss: 0.787\n",
            "[9,    15] loss: 0.848\n",
            "[9,    30] loss: 0.793\n",
            "[9,    45] loss: 0.764\n",
            "[9,    60] loss: 0.759\n",
            "[9,    75] loss: 0.789\n",
            "[10,    15] loss: 0.757\n",
            "[10,    30] loss: 0.740\n",
            "[10,    45] loss: 0.721\n",
            "[10,    60] loss: 0.729\n",
            "[10,    75] loss: 0.711\n",
            "[11,    15] loss: 0.794\n",
            "[11,    30] loss: 0.691\n",
            "[11,    45] loss: 0.702\n",
            "[11,    60] loss: 0.705\n",
            "[11,    75] loss: 0.655\n",
            "[12,    15] loss: 0.684\n",
            "[12,    30] loss: 0.635\n",
            "[12,    45] loss: 0.669\n",
            "[12,    60] loss: 0.681\n",
            "[12,    75] loss: 0.656\n",
            "[13,    15] loss: 0.688\n",
            "[13,    30] loss: 0.655\n",
            "[13,    45] loss: 0.667\n",
            "[13,    60] loss: 0.640\n",
            "[13,    75] loss: 0.622\n",
            "[14,    15] loss: 0.678\n",
            "[14,    30] loss: 0.608\n",
            "[14,    45] loss: 0.598\n",
            "[14,    60] loss: 0.576\n",
            "[14,    75] loss: 0.639\n",
            "[15,    15] loss: 0.581\n",
            "[15,    30] loss: 0.590\n",
            "[15,    45] loss: 0.599\n",
            "[15,    60] loss: 0.587\n",
            "[15,    75] loss: 0.587\n",
            "[16,    15] loss: 0.597\n",
            "[16,    30] loss: 0.543\n",
            "[16,    45] loss: 0.571\n",
            "[16,    60] loss: 0.558\n",
            "[16,    75] loss: 0.547\n",
            "[17,    15] loss: 0.650\n",
            "[17,    30] loss: 0.497\n",
            "[17,    45] loss: 0.525\n",
            "[17,    60] loss: 0.513\n",
            "[17,    75] loss: 0.565\n",
            "[18,    15] loss: 0.563\n",
            "[18,    30] loss: 0.512\n",
            "[18,    45] loss: 0.520\n",
            "[18,    60] loss: 0.502\n",
            "[18,    75] loss: 0.511\n",
            "[19,    15] loss: 0.510\n",
            "[19,    30] loss: 0.495\n",
            "[19,    45] loss: 0.498\n",
            "[19,    60] loss: 0.473\n",
            "[19,    75] loss: 0.499\n",
            "[20,    15] loss: 0.510\n",
            "[20,    30] loss: 0.481\n",
            "[20,    45] loss: 0.460\n",
            "[20,    60] loss: 0.470\n",
            "[20,    75] loss: 0.414\n",
            "Accuracy of the network: 77.58 %\n",
            "l=5; n=350\n",
            "[1,    15] loss: 2.303\n",
            "[1,    30] loss: 2.301\n",
            "[1,    45] loss: 2.299\n",
            "[1,    60] loss: 2.298\n",
            "[1,    75] loss: 2.296\n",
            "[2,    15] loss: 2.292\n",
            "[2,    30] loss: 2.289\n",
            "[2,    45] loss: 2.285\n",
            "[2,    60] loss: 2.280\n",
            "[2,    75] loss: 2.273\n",
            "[3,    15] loss: 2.260\n",
            "[3,    30] loss: 2.243\n",
            "[3,    45] loss: 2.220\n",
            "[3,    60] loss: 2.174\n",
            "[3,    75] loss: 2.091\n",
            "[4,    15] loss: 1.903\n",
            "[4,    30] loss: 1.655\n",
            "[4,    45] loss: 1.431\n",
            "[4,    60] loss: 1.304\n",
            "[4,    75] loss: 1.227\n",
            "[5,    15] loss: 1.145\n",
            "[5,    30] loss: 1.068\n",
            "[5,    45] loss: 1.049\n",
            "[5,    60] loss: 1.006\n",
            "[5,    75] loss: 0.983\n",
            "[6,    15] loss: 0.969\n",
            "[6,    30] loss: 0.896\n",
            "[6,    45] loss: 0.892\n",
            "[6,    60] loss: 0.874\n",
            "[6,    75] loss: 0.850\n",
            "[7,    15] loss: 0.845\n",
            "[7,    30] loss: 0.859\n",
            "[7,    45] loss: 0.867\n",
            "[7,    60] loss: 0.823\n",
            "[7,    75] loss: 0.794\n",
            "[8,    15] loss: 0.810\n",
            "[8,    30] loss: 0.799\n",
            "[8,    45] loss: 0.762\n",
            "[8,    60] loss: 0.779\n",
            "[8,    75] loss: 0.767\n",
            "[9,    15] loss: 0.737\n",
            "[9,    30] loss: 0.764\n",
            "[9,    45] loss: 0.717\n",
            "[9,    60] loss: 0.704\n",
            "[9,    75] loss: 0.701\n",
            "[10,    15] loss: 0.643\n",
            "[10,    30] loss: 0.681\n",
            "[10,    45] loss: 0.700\n",
            "[10,    60] loss: 0.642\n",
            "[10,    75] loss: 0.690\n",
            "[11,    15] loss: 0.691\n",
            "[11,    30] loss: 0.661\n",
            "[11,    45] loss: 0.650\n",
            "[11,    60] loss: 0.623\n",
            "[11,    75] loss: 0.645\n",
            "[12,    15] loss: 0.664\n",
            "[12,    30] loss: 0.611\n",
            "[12,    45] loss: 0.609\n",
            "[12,    60] loss: 0.590\n",
            "[12,    75] loss: 0.621\n",
            "[13,    15] loss: 0.611\n",
            "[13,    30] loss: 0.576\n",
            "[13,    45] loss: 0.594\n",
            "[13,    60] loss: 0.554\n",
            "[13,    75] loss: 0.552\n",
            "[14,    15] loss: 0.573\n",
            "[14,    30] loss: 0.575\n",
            "[14,    45] loss: 0.496\n",
            "[14,    60] loss: 0.575\n",
            "[14,    75] loss: 0.553\n",
            "[15,    15] loss: 0.572\n",
            "[15,    30] loss: 0.518\n",
            "[15,    45] loss: 0.507\n",
            "[15,    60] loss: 0.518\n",
            "[15,    75] loss: 0.523\n",
            "[16,    15] loss: 0.529\n",
            "[16,    30] loss: 0.500\n",
            "[16,    45] loss: 0.498\n",
            "[16,    60] loss: 0.472\n",
            "[16,    75] loss: 0.475\n",
            "[17,    15] loss: 0.492\n",
            "[17,    30] loss: 0.506\n",
            "[17,    45] loss: 0.479\n",
            "[17,    60] loss: 0.457\n",
            "[17,    75] loss: 0.484\n",
            "[18,    15] loss: 0.437\n",
            "[18,    30] loss: 0.459\n",
            "[18,    45] loss: 0.441\n",
            "[18,    60] loss: 0.442\n",
            "[18,    75] loss: 0.436\n",
            "[19,    15] loss: 0.534\n",
            "[19,    30] loss: 0.473\n",
            "[19,    45] loss: 0.431\n",
            "[19,    60] loss: 0.415\n",
            "[19,    75] loss: 0.409\n",
            "[20,    15] loss: 0.458\n",
            "[20,    30] loss: 0.463\n",
            "[20,    45] loss: 0.436\n",
            "[20,    60] loss: 0.387\n",
            "[20,    75] loss: 0.364\n",
            "Accuracy of the network: 81.22 %\n",
            "l=5; n=500\n",
            "[1,    15] loss: 2.302\n",
            "[1,    30] loss: 2.300\n",
            "[1,    45] loss: 2.297\n",
            "[1,    60] loss: 2.295\n",
            "[1,    75] loss: 2.291\n",
            "[2,    15] loss: 2.287\n",
            "[2,    30] loss: 2.281\n",
            "[2,    45] loss: 2.273\n",
            "[2,    60] loss: 2.264\n",
            "[2,    75] loss: 2.247\n",
            "[3,    15] loss: 2.212\n",
            "[3,    30] loss: 2.157\n",
            "[3,    45] loss: 2.056\n",
            "[3,    60] loss: 1.852\n",
            "[3,    75] loss: 1.598\n",
            "[4,    15] loss: 1.361\n",
            "[4,    30] loss: 1.213\n",
            "[4,    45] loss: 1.177\n",
            "[4,    60] loss: 1.157\n",
            "[4,    75] loss: 1.045\n",
            "[5,    15] loss: 1.002\n",
            "[5,    30] loss: 0.977\n",
            "[5,    45] loss: 0.886\n",
            "[5,    60] loss: 0.941\n",
            "[5,    75] loss: 0.924\n",
            "[6,    15] loss: 0.948\n",
            "[6,    30] loss: 0.837\n",
            "[6,    45] loss: 0.829\n",
            "[6,    60] loss: 0.844\n",
            "[6,    75] loss: 0.817\n",
            "[7,    15] loss: 0.813\n",
            "[7,    30] loss: 0.783\n",
            "[7,    45] loss: 0.811\n",
            "[7,    60] loss: 0.742\n",
            "[7,    75] loss: 0.778\n",
            "[8,    15] loss: 0.795\n",
            "[8,    30] loss: 0.726\n",
            "[8,    45] loss: 0.744\n",
            "[8,    60] loss: 0.760\n",
            "[8,    75] loss: 0.709\n",
            "[9,    15] loss: 0.741\n",
            "[9,    30] loss: 0.708\n",
            "[9,    45] loss: 0.705\n",
            "[9,    60] loss: 0.648\n",
            "[9,    75] loss: 0.650\n",
            "[10,    15] loss: 0.675\n",
            "[10,    30] loss: 0.654\n",
            "[10,    45] loss: 0.622\n",
            "[10,    60] loss: 0.624\n",
            "[10,    75] loss: 0.634\n",
            "[11,    15] loss: 0.620\n",
            "[11,    30] loss: 0.631\n",
            "[11,    45] loss: 0.612\n",
            "[11,    60] loss: 0.572\n",
            "[11,    75] loss: 0.560\n",
            "[12,    15] loss: 0.600\n",
            "[12,    30] loss: 0.543\n",
            "[12,    45] loss: 0.533\n",
            "[12,    60] loss: 0.560\n",
            "[12,    75] loss: 0.616\n",
            "[13,    15] loss: 0.580\n",
            "[13,    30] loss: 0.528\n",
            "[13,    45] loss: 0.475\n",
            "[13,    60] loss: 0.575\n",
            "[13,    75] loss: 0.514\n",
            "[14,    15] loss: 0.490\n",
            "[14,    30] loss: 0.486\n",
            "[14,    45] loss: 0.485\n",
            "[14,    60] loss: 0.530\n",
            "[14,    75] loss: 0.519\n",
            "[15,    15] loss: 0.488\n",
            "[15,    30] loss: 0.462\n",
            "[15,    45] loss: 0.464\n",
            "[15,    60] loss: 0.498\n",
            "[15,    75] loss: 0.483\n",
            "[16,    15] loss: 0.554\n",
            "[16,    30] loss: 0.446\n",
            "[16,    45] loss: 0.478\n",
            "[16,    60] loss: 0.467\n",
            "[16,    75] loss: 0.452\n",
            "[17,    15] loss: 0.454\n",
            "[17,    30] loss: 0.406\n",
            "[17,    45] loss: 0.479\n",
            "[17,    60] loss: 0.473\n",
            "[17,    75] loss: 0.413\n",
            "[18,    15] loss: 0.436\n",
            "[18,    30] loss: 0.442\n",
            "[18,    45] loss: 0.437\n",
            "[18,    60] loss: 0.411\n",
            "[18,    75] loss: 0.468\n",
            "[19,    15] loss: 0.460\n",
            "[19,    30] loss: 0.413\n",
            "[19,    45] loss: 0.381\n",
            "[19,    60] loss: 0.407\n",
            "[19,    75] loss: 0.441\n",
            "[20,    15] loss: 0.464\n",
            "[20,    30] loss: 0.369\n",
            "[20,    45] loss: 0.391\n",
            "[20,    60] loss: 0.424\n",
            "[20,    75] loss: 0.368\n",
            "Accuracy of the network: 75.7 %\n",
            "l=6; n=50\n",
            "[1,    15] loss: 2.309\n",
            "[1,    30] loss: 2.305\n",
            "[1,    45] loss: 2.304\n",
            "[1,    60] loss: 2.301\n",
            "[1,    75] loss: 2.303\n",
            "[2,    15] loss: 2.302\n",
            "[2,    30] loss: 2.302\n",
            "[2,    45] loss: 2.302\n",
            "[2,    60] loss: 2.304\n",
            "[2,    75] loss: 2.303\n",
            "[3,    15] loss: 2.301\n",
            "[3,    30] loss: 2.302\n",
            "[3,    45] loss: 2.303\n",
            "[3,    60] loss: 2.301\n",
            "[3,    75] loss: 2.301\n",
            "[4,    15] loss: 2.301\n",
            "[4,    30] loss: 2.300\n",
            "[4,    45] loss: 2.301\n",
            "[4,    60] loss: 2.301\n",
            "[4,    75] loss: 2.300\n",
            "[5,    15] loss: 2.299\n",
            "[5,    30] loss: 2.301\n",
            "[5,    45] loss: 2.300\n",
            "[5,    60] loss: 2.299\n",
            "[5,    75] loss: 2.299\n",
            "[6,    15] loss: 2.298\n",
            "[6,    30] loss: 2.298\n",
            "[6,    45] loss: 2.298\n",
            "[6,    60] loss: 2.298\n",
            "[6,    75] loss: 2.297\n",
            "[7,    15] loss: 2.297\n",
            "[7,    30] loss: 2.296\n",
            "[7,    45] loss: 2.294\n",
            "[7,    60] loss: 2.294\n",
            "[7,    75] loss: 2.292\n",
            "[8,    15] loss: 2.291\n",
            "[8,    30] loss: 2.289\n",
            "[8,    45] loss: 2.286\n",
            "[8,    60] loss: 2.283\n",
            "[8,    75] loss: 2.282\n",
            "[9,    15] loss: 2.274\n",
            "[9,    30] loss: 2.268\n",
            "[9,    45] loss: 2.259\n",
            "[9,    60] loss: 2.248\n",
            "[9,    75] loss: 2.232\n",
            "[10,    15] loss: 2.199\n",
            "[10,    30] loss: 2.156\n",
            "[10,    45] loss: 2.084\n",
            "[10,    60] loss: 1.977\n",
            "[10,    75] loss: 1.830\n",
            "[11,    15] loss: 1.670\n",
            "[11,    30] loss: 1.535\n",
            "[11,    45] loss: 1.367\n",
            "[11,    60] loss: 1.272\n",
            "[11,    75] loss: 1.223\n",
            "[12,    15] loss: 1.327\n",
            "[12,    30] loss: 1.127\n",
            "[12,    45] loss: 1.126\n",
            "[12,    60] loss: 1.074\n",
            "[12,    75] loss: 1.085\n",
            "[13,    15] loss: 1.090\n",
            "[13,    30] loss: 1.040\n",
            "[13,    45] loss: 1.045\n",
            "[13,    60] loss: 1.111\n",
            "[13,    75] loss: 1.031\n",
            "[14,    15] loss: 0.967\n",
            "[14,    30] loss: 0.972\n",
            "[14,    45] loss: 0.967\n",
            "[14,    60] loss: 0.948\n",
            "[14,    75] loss: 0.964\n",
            "[15,    15] loss: 1.019\n",
            "[15,    30] loss: 0.885\n",
            "[15,    45] loss: 0.911\n",
            "[15,    60] loss: 0.955\n",
            "[15,    75] loss: 0.895\n",
            "[16,    15] loss: 0.865\n",
            "[16,    30] loss: 0.891\n",
            "[16,    45] loss: 0.869\n",
            "[16,    60] loss: 0.859\n",
            "[16,    75] loss: 0.834\n",
            "[17,    15] loss: 0.846\n",
            "[17,    30] loss: 0.788\n",
            "[17,    45] loss: 0.908\n",
            "[17,    60] loss: 0.860\n",
            "[17,    75] loss: 0.804\n",
            "[18,    15] loss: 0.800\n",
            "[18,    30] loss: 0.765\n",
            "[18,    45] loss: 0.795\n",
            "[18,    60] loss: 0.788\n",
            "[18,    75] loss: 0.740\n",
            "[19,    15] loss: 0.784\n",
            "[19,    30] loss: 0.737\n",
            "[19,    45] loss: 0.712\n",
            "[19,    60] loss: 0.759\n",
            "[19,    75] loss: 0.747\n",
            "[20,    15] loss: 0.794\n",
            "[20,    30] loss: 0.671\n",
            "[20,    45] loss: 0.756\n",
            "[20,    60] loss: 0.712\n",
            "[20,    75] loss: 0.747\n",
            "Accuracy of the network: 75.49 %\n",
            "l=6; n=100\n",
            "[1,    15] loss: 2.304\n",
            "[1,    30] loss: 2.303\n",
            "[1,    45] loss: 2.304\n",
            "[1,    60] loss: 2.303\n",
            "[1,    75] loss: 2.302\n",
            "[2,    15] loss: 2.303\n",
            "[2,    30] loss: 2.302\n",
            "[2,    45] loss: 2.301\n",
            "[2,    60] loss: 2.301\n",
            "[2,    75] loss: 2.301\n",
            "[3,    15] loss: 2.301\n",
            "[3,    30] loss: 2.300\n",
            "[3,    45] loss: 2.300\n",
            "[3,    60] loss: 2.300\n",
            "[3,    75] loss: 2.299\n",
            "[4,    15] loss: 2.298\n",
            "[4,    30] loss: 2.299\n",
            "[4,    45] loss: 2.298\n",
            "[4,    60] loss: 2.297\n",
            "[4,    75] loss: 2.297\n",
            "[5,    15] loss: 2.295\n",
            "[5,    30] loss: 2.294\n",
            "[5,    45] loss: 2.293\n",
            "[5,    60] loss: 2.293\n",
            "[5,    75] loss: 2.290\n",
            "[6,    15] loss: 2.286\n",
            "[6,    30] loss: 2.285\n",
            "[6,    45] loss: 2.280\n",
            "[6,    60] loss: 2.275\n",
            "[6,    75] loss: 2.268\n",
            "[7,    15] loss: 2.255\n",
            "[7,    30] loss: 2.240\n",
            "[7,    45] loss: 2.215\n",
            "[7,    60] loss: 2.179\n",
            "[7,    75] loss: 2.102\n",
            "[8,    15] loss: 1.935\n",
            "[8,    30] loss: 1.726\n",
            "[8,    45] loss: 1.533\n",
            "[8,    60] loss: 1.389\n",
            "[8,    75] loss: 1.284\n",
            "[9,    15] loss: 1.211\n",
            "[9,    30] loss: 1.149\n",
            "[9,    45] loss: 1.166\n",
            "[9,    60] loss: 1.118\n",
            "[9,    75] loss: 1.101\n",
            "[10,    15] loss: 1.125\n",
            "[10,    30] loss: 1.140\n",
            "[10,    45] loss: 1.100\n",
            "[10,    60] loss: 1.065\n",
            "[10,    75] loss: 0.996\n",
            "[11,    15] loss: 1.047\n",
            "[11,    30] loss: 0.962\n",
            "[11,    45] loss: 1.025\n",
            "[11,    60] loss: 0.977\n",
            "[11,    75] loss: 0.917\n",
            "[12,    15] loss: 1.015\n",
            "[12,    30] loss: 0.960\n",
            "[12,    45] loss: 0.989\n",
            "[12,    60] loss: 0.930\n",
            "[12,    75] loss: 0.876\n",
            "[13,    15] loss: 0.985\n",
            "[13,    30] loss: 0.913\n",
            "[13,    45] loss: 0.920\n",
            "[13,    60] loss: 0.915\n",
            "[13,    75] loss: 0.846\n",
            "[14,    15] loss: 0.896\n",
            "[14,    30] loss: 0.834\n",
            "[14,    45] loss: 0.904\n",
            "[14,    60] loss: 0.859\n",
            "[14,    75] loss: 0.778\n",
            "[15,    15] loss: 0.808\n",
            "[15,    30] loss: 0.770\n",
            "[15,    45] loss: 0.811\n",
            "[15,    60] loss: 0.871\n",
            "[15,    75] loss: 0.757\n",
            "[16,    15] loss: 0.824\n",
            "[16,    30] loss: 0.738\n",
            "[16,    45] loss: 0.710\n",
            "[16,    60] loss: 0.748\n",
            "[16,    75] loss: 0.753\n",
            "[17,    15] loss: 0.829\n",
            "[17,    30] loss: 0.678\n",
            "[17,    45] loss: 0.778\n",
            "[17,    60] loss: 0.705\n",
            "[17,    75] loss: 0.684\n",
            "[18,    15] loss: 0.828\n",
            "[18,    30] loss: 0.673\n",
            "[18,    45] loss: 0.681\n",
            "[18,    60] loss: 0.609\n",
            "[18,    75] loss: 0.618\n",
            "[19,    15] loss: 0.670\n",
            "[19,    30] loss: 0.663\n",
            "[19,    45] loss: 0.614\n",
            "[19,    60] loss: 0.587\n",
            "[19,    75] loss: 0.654\n",
            "[20,    15] loss: 0.620\n",
            "[20,    30] loss: 0.620\n",
            "[20,    45] loss: 0.572\n",
            "[20,    60] loss: 0.626\n",
            "[20,    75] loss: 0.597\n",
            "Accuracy of the network: 67.97 %\n",
            "l=6; n=200\n",
            "[1,    15] loss: 2.303\n",
            "[1,    30] loss: 2.303\n",
            "[1,    45] loss: 2.304\n",
            "[1,    60] loss: 2.301\n",
            "[1,    75] loss: 2.301\n",
            "[2,    15] loss: 2.301\n",
            "[2,    30] loss: 2.301\n",
            "[2,    45] loss: 2.300\n",
            "[2,    60] loss: 2.300\n",
            "[2,    75] loss: 2.301\n",
            "[3,    15] loss: 2.299\n",
            "[3,    30] loss: 2.299\n",
            "[3,    45] loss: 2.299\n",
            "[3,    60] loss: 2.298\n",
            "[3,    75] loss: 2.298\n",
            "[4,    15] loss: 2.296\n",
            "[4,    30] loss: 2.296\n",
            "[4,    45] loss: 2.295\n",
            "[4,    60] loss: 2.295\n",
            "[4,    75] loss: 2.293\n",
            "[5,    15] loss: 2.291\n",
            "[5,    30] loss: 2.288\n",
            "[5,    45] loss: 2.287\n",
            "[5,    60] loss: 2.282\n",
            "[5,    75] loss: 2.278\n",
            "[6,    15] loss: 2.271\n",
            "[6,    30] loss: 2.263\n",
            "[6,    45] loss: 2.252\n",
            "[6,    60] loss: 2.236\n",
            "[6,    75] loss: 2.209\n",
            "[7,    15] loss: 2.155\n",
            "[7,    30] loss: 2.058\n",
            "[7,    45] loss: 1.916\n",
            "[7,    60] loss: 1.716\n",
            "[7,    75] loss: 1.548\n",
            "[8,    15] loss: 1.348\n",
            "[8,    30] loss: 1.258\n",
            "[8,    45] loss: 1.197\n",
            "[8,    60] loss: 1.215\n",
            "[8,    75] loss: 1.132\n",
            "[9,    15] loss: 1.193\n",
            "[9,    30] loss: 1.049\n",
            "[9,    45] loss: 1.173\n",
            "[9,    60] loss: 1.015\n",
            "[9,    75] loss: 0.999\n",
            "[10,    15] loss: 1.168\n",
            "[10,    30] loss: 1.003\n",
            "[10,    45] loss: 1.013\n",
            "[10,    60] loss: 1.029\n",
            "[10,    75] loss: 0.970\n",
            "[11,    15] loss: 0.996\n",
            "[11,    30] loss: 0.964\n",
            "[11,    45] loss: 0.914\n",
            "[11,    60] loss: 0.865\n",
            "[11,    75] loss: 0.925\n",
            "[12,    15] loss: 0.922\n",
            "[12,    30] loss: 0.855\n",
            "[12,    45] loss: 0.861\n",
            "[12,    60] loss: 0.892\n",
            "[12,    75] loss: 0.894\n",
            "[13,    15] loss: 0.842\n",
            "[13,    30] loss: 0.780\n",
            "[13,    45] loss: 0.815\n",
            "[13,    60] loss: 0.804\n",
            "[13,    75] loss: 0.810\n",
            "[14,    15] loss: 0.753\n",
            "[14,    30] loss: 0.753\n",
            "[14,    45] loss: 0.752\n",
            "[14,    60] loss: 0.727\n",
            "[14,    75] loss: 0.721\n",
            "[15,    15] loss: 0.743\n",
            "[15,    30] loss: 0.673\n",
            "[15,    45] loss: 0.710\n",
            "[15,    60] loss: 0.665\n",
            "[15,    75] loss: 0.670\n",
            "[16,    15] loss: 0.686\n",
            "[16,    30] loss: 0.656\n",
            "[16,    45] loss: 0.643\n",
            "[16,    60] loss: 0.662\n",
            "[16,    75] loss: 0.662\n",
            "[17,    15] loss: 0.691\n",
            "[17,    30] loss: 0.606\n",
            "[17,    45] loss: 0.593\n",
            "[17,    60] loss: 0.647\n",
            "[17,    75] loss: 0.757\n",
            "[18,    15] loss: 0.592\n",
            "[18,    30] loss: 0.587\n",
            "[18,    45] loss: 0.596\n",
            "[18,    60] loss: 0.586\n",
            "[18,    75] loss: 0.596\n",
            "[19,    15] loss: 0.601\n",
            "[19,    30] loss: 0.605\n",
            "[19,    45] loss: 0.597\n",
            "[19,    60] loss: 0.568\n",
            "[19,    75] loss: 0.581\n",
            "[20,    15] loss: 0.569\n",
            "[20,    30] loss: 0.534\n",
            "[20,    45] loss: 0.547\n",
            "[20,    60] loss: 0.561\n",
            "[20,    75] loss: 0.531\n",
            "Accuracy of the network: 75.9 %\n",
            "l=6; n=350\n",
            "[1,    15] loss: 2.302\n",
            "[1,    30] loss: 2.302\n",
            "[1,    45] loss: 2.302\n",
            "[1,    60] loss: 2.302\n",
            "[1,    75] loss: 2.301\n",
            "[2,    15] loss: 2.300\n",
            "[2,    30] loss: 2.301\n",
            "[2,    45] loss: 2.300\n",
            "[2,    60] loss: 2.299\n",
            "[2,    75] loss: 2.298\n",
            "[3,    15] loss: 2.297\n",
            "[3,    30] loss: 2.297\n",
            "[3,    45] loss: 2.296\n",
            "[3,    60] loss: 2.295\n",
            "[3,    75] loss: 2.295\n",
            "[4,    15] loss: 2.293\n",
            "[4,    30] loss: 2.291\n",
            "[4,    45] loss: 2.288\n",
            "[4,    60] loss: 2.286\n",
            "[4,    75] loss: 2.282\n",
            "[5,    15] loss: 2.277\n",
            "[5,    30] loss: 2.271\n",
            "[5,    45] loss: 2.264\n",
            "[5,    60] loss: 2.251\n",
            "[5,    75] loss: 2.234\n",
            "[6,    15] loss: 2.194\n",
            "[6,    30] loss: 2.126\n",
            "[6,    45] loss: 2.024\n",
            "[6,    60] loss: 1.828\n",
            "[6,    75] loss: 1.584\n",
            "[7,    15] loss: 1.350\n",
            "[7,    30] loss: 1.231\n",
            "[7,    45] loss: 1.198\n",
            "[7,    60] loss: 1.142\n",
            "[7,    75] loss: 1.203\n",
            "[8,    15] loss: 1.094\n",
            "[8,    30] loss: 1.087\n",
            "[8,    45] loss: 1.122\n",
            "[8,    60] loss: 1.041\n",
            "[8,    75] loss: 0.976\n",
            "[9,    15] loss: 1.000\n",
            "[9,    30] loss: 0.940\n",
            "[9,    45] loss: 0.912\n",
            "[9,    60] loss: 0.990\n",
            "[9,    75] loss: 0.970\n",
            "[10,    15] loss: 0.854\n",
            "[10,    30] loss: 0.839\n",
            "[10,    45] loss: 0.857\n",
            "[10,    60] loss: 0.828\n",
            "[10,    75] loss: 0.826\n",
            "[11,    15] loss: 0.743\n",
            "[11,    30] loss: 0.839\n",
            "[11,    45] loss: 0.803\n",
            "[11,    60] loss: 0.789\n",
            "[11,    75] loss: 0.788\n",
            "[12,    15] loss: 0.789\n",
            "[12,    30] loss: 0.743\n",
            "[12,    45] loss: 0.749\n",
            "[12,    60] loss: 0.718\n",
            "[12,    75] loss: 0.731\n",
            "[13,    15] loss: 0.706\n",
            "[13,    30] loss: 0.696\n",
            "[13,    45] loss: 0.714\n",
            "[13,    60] loss: 0.679\n",
            "[13,    75] loss: 0.688\n",
            "[14,    15] loss: 0.704\n",
            "[14,    30] loss: 0.690\n",
            "[14,    45] loss: 0.709\n",
            "[14,    60] loss: 0.666\n",
            "[14,    75] loss: 0.653\n",
            "[15,    15] loss: 0.659\n",
            "[15,    30] loss: 0.592\n",
            "[15,    45] loss: 0.622\n",
            "[15,    60] loss: 0.618\n",
            "[15,    75] loss: 0.597\n",
            "[16,    15] loss: 0.657\n",
            "[16,    30] loss: 0.607\n",
            "[16,    45] loss: 0.579\n",
            "[16,    60] loss: 0.557\n",
            "[16,    75] loss: 0.583\n",
            "[17,    15] loss: 0.573\n",
            "[17,    30] loss: 0.581\n",
            "[17,    45] loss: 0.572\n",
            "[17,    60] loss: 0.581\n",
            "[17,    75] loss: 0.551\n",
            "[18,    15] loss: 0.525\n",
            "[18,    30] loss: 0.512\n",
            "[18,    45] loss: 0.572\n",
            "[18,    60] loss: 0.565\n",
            "[18,    75] loss: 0.578\n",
            "[19,    15] loss: 0.540\n",
            "[19,    30] loss: 0.526\n",
            "[19,    45] loss: 0.563\n",
            "[19,    60] loss: 0.572\n",
            "[19,    75] loss: 0.509\n",
            "[20,    15] loss: 0.503\n",
            "[20,    30] loss: 0.533\n",
            "[20,    45] loss: 0.518\n",
            "[20,    60] loss: 0.480\n",
            "[20,    75] loss: 0.473\n",
            "Accuracy of the network: 63.04 %\n",
            "l=6; n=500\n",
            "[1,    15] loss: 2.302\n",
            "[1,    30] loss: 2.303\n",
            "[1,    45] loss: 2.301\n",
            "[1,    60] loss: 2.301\n",
            "[1,    75] loss: 2.300\n",
            "[2,    15] loss: 2.299\n",
            "[2,    30] loss: 2.298\n",
            "[2,    45] loss: 2.298\n",
            "[2,    60] loss: 2.297\n",
            "[2,    75] loss: 2.296\n",
            "[3,    15] loss: 2.295\n",
            "[3,    30] loss: 2.293\n",
            "[3,    45] loss: 2.291\n",
            "[3,    60] loss: 2.289\n",
            "[3,    75] loss: 2.286\n",
            "[4,    15] loss: 2.281\n",
            "[4,    30] loss: 2.274\n",
            "[4,    45] loss: 2.268\n",
            "[4,    60] loss: 2.255\n",
            "[4,    75] loss: 2.239\n",
            "[5,    15] loss: 2.198\n",
            "[5,    30] loss: 2.139\n",
            "[5,    45] loss: 2.039\n",
            "[5,    60] loss: 1.885\n",
            "[5,    75] loss: 1.696\n",
            "[6,    15] loss: 1.532\n",
            "[6,    30] loss: 1.403\n",
            "[6,    45] loss: 1.290\n",
            "[6,    60] loss: 1.190\n",
            "[6,    75] loss: 1.156\n",
            "[7,    15] loss: 1.099\n",
            "[7,    30] loss: 1.071\n",
            "[7,    45] loss: 1.039\n",
            "[7,    60] loss: 1.015\n",
            "[7,    75] loss: 1.004\n",
            "[8,    15] loss: 1.187\n",
            "[8,    30] loss: 0.968\n",
            "[8,    45] loss: 0.934\n",
            "[8,    60] loss: 0.870\n",
            "[8,    75] loss: 0.886\n",
            "[9,    15] loss: 0.888\n",
            "[9,    30] loss: 0.797\n",
            "[9,    45] loss: 0.899\n",
            "[9,    60] loss: 0.881\n",
            "[9,    75] loss: 0.878\n",
            "[10,    15] loss: 0.828\n",
            "[10,    30] loss: 0.829\n",
            "[10,    45] loss: 0.788\n",
            "[10,    60] loss: 0.799\n",
            "[10,    75] loss: 0.772\n",
            "[11,    15] loss: 0.810\n",
            "[11,    30] loss: 0.720\n",
            "[11,    45] loss: 0.698\n",
            "[11,    60] loss: 0.822\n",
            "[11,    75] loss: 0.762\n",
            "[12,    15] loss: 0.840\n",
            "[12,    30] loss: 0.708\n",
            "[12,    45] loss: 0.661\n",
            "[12,    60] loss: 0.658\n",
            "[12,    75] loss: 0.645\n",
            "[13,    15] loss: 0.665\n",
            "[13,    30] loss: 0.667\n",
            "[13,    45] loss: 0.622\n",
            "[13,    60] loss: 0.635\n",
            "[13,    75] loss: 0.675\n",
            "[14,    15] loss: 0.675\n",
            "[14,    30] loss: 0.636\n",
            "[14,    45] loss: 0.580\n",
            "[14,    60] loss: 0.620\n",
            "[14,    75] loss: 0.619\n",
            "[15,    15] loss: 0.565\n",
            "[15,    30] loss: 0.596\n",
            "[15,    45] loss: 0.601\n",
            "[15,    60] loss: 0.582\n",
            "[15,    75] loss: 0.611\n",
            "[16,    15] loss: 0.583\n",
            "[16,    30] loss: 0.550\n",
            "[16,    45] loss: 0.554\n",
            "[16,    60] loss: 0.564\n",
            "[16,    75] loss: 0.527\n",
            "[17,    15] loss: 0.532\n",
            "[17,    30] loss: 0.521\n",
            "[17,    45] loss: 0.561\n",
            "[17,    60] loss: 0.549\n",
            "[17,    75] loss: 0.583\n",
            "[18,    15] loss: 0.523\n",
            "[18,    30] loss: 0.518\n",
            "[18,    45] loss: 0.532\n",
            "[18,    60] loss: 0.516\n",
            "[18,    75] loss: 0.555\n",
            "[19,    15] loss: 0.476\n",
            "[19,    30] loss: 0.499\n",
            "[19,    45] loss: 0.467\n",
            "[19,    60] loss: 0.518\n",
            "[19,    75] loss: 0.531\n",
            "[20,    15] loss: 0.557\n",
            "[20,    30] loss: 0.473\n",
            "[20,    45] loss: 0.460\n",
            "[20,    60] loss: 0.465\n",
            "[20,    75] loss: 0.451\n",
            "Accuracy of the network: 80.98 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "При большем количестве слоев ошибка при обучении не уменьшается. Так же замечено, что при увеличении числа нейронов в слоях accuracy практически не изменяется."
      ],
      "metadata": {
        "id": "WNt_dtyLK6EH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "n_neurons = len(neurons)\n",
        "n_layers = len(layers)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "x = neurons\n",
        "for i in range(0, n_layers):\n",
        "    y = accurs[i*n_layers:(i+1)*n_layers]\n",
        "    plt.plot(x, y, label=f'layers={layers[i]}')\n",
        "plt.xlabel('neurons')\n",
        "plt.ylabel('accuracy')\n",
        "\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "nmlT2Jz5yPYq",
        "outputId": "69cc53e4-1f73-467a-b9f9-32b83e55b83c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4Jv6T9oSjXY"
      },
      "source": [
        "### Task 3: Fix it.\n",
        "Fix the overfitted network from the previous step (at least partially) by using regularization techniques (Dropout/Batchnorm/...) and demonstrate the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jrod66qSjXY"
      },
      "outputs": [],
      "source": [
        "class FixedNeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_shape=28*28, num_classes=10, input_channels=1):\n",
        "        super(self.__class__, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Flatten(), # This layer converts image into a vector to use Linear layers afterwards\n",
        "            # Your network structure comes here\n",
        "            nn.Linear(input_shape, 350),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.BatchNorm1d(350),\n",
        "            nn.Linear(350, num_classes),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, inp):\n",
        "        out = self.model(inp)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbH1z4sFSjXZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1343254-9586-4251-802b-f5d4348ba5b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "           Flatten-1                  [-1, 784]               0\n",
            "            Linear-2                  [-1, 350]         274,750\n",
            "              ReLU-3                  [-1, 350]               0\n",
            "           Dropout-4                  [-1, 350]               0\n",
            "       BatchNorm1d-5                  [-1, 350]             700\n",
            "            Linear-6                   [-1, 10]           3,510\n",
            "              ReLU-7                   [-1, 10]               0\n",
            "================================================================\n",
            "Total params: 278,960\n",
            "Trainable params: 278,960\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.02\n",
            "Params size (MB): 1.06\n",
            "Estimated Total Size (MB): 1.08\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "torchsummary.summary(FixedNeuralNetwork().to(device), (28*28,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljfSX1G_SjXZ"
      },
      "outputs": [],
      "source": [
        "model = FixedNeuralNetwork().to(device)\n",
        "opt = torch.optim.SGD(model.parameters(), lr=1e-1, weight_decay=1e-5)\n",
        "loss_func = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import DataLoader\n",
        "num_val_samples = 10000\n",
        "num_train_samples = len(train_loader.dataset) - num_val_samples\n",
        "val_dataset, new_train_dataset = random_split(train_loader.dataset, [num_val_samples, num_train_samples])\n",
        "new_train_loader = DataLoader(new_train_dataset, batch_size=128, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "id": "gODTf4W_aNYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_validation_loss(model, validation_dataloader, criterion):\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    num_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in validation_dataloader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            num_samples += 1\n",
        "\n",
        "    average_loss = total_loss / num_samples\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    return average_loss"
      ],
      "metadata": {
        "id": "sGWw1lDNdIgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FixedNeuralNetworkTmp(nn.Module):\n",
        "    def __init__(self, input_shape=28*28, num_classes=10, input_channels=1):\n",
        "        super(self.__class__, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Flatten(), # This layer converts image into a vector to use Linear layers afterwards\n",
        "            # Your network structure comes here\n",
        "            nn.Linear(input_shape, 500),\n",
        "            nn.BatchNorm1d(500),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.05),\n",
        "            nn.Linear(500, 250),\n",
        "            nn.BatchNorm1d(250),\n",
        "            nn.GLU(),\n",
        "            nn.Dropout(0.05),\n",
        "            nn.Linear(125, 2*num_classes),\n",
        "            nn.GLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, inp):\n",
        "        out = self.model(inp)\n",
        "        return out"
      ],
      "metadata": {
        "id": "HCX9dDch_jxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import lr_scheduler\n",
        "\n",
        "def train(net, trainloader, val_loader, criterion, optimizer, epochs=5, device='cpu', patience=4, printing=True):\n",
        "    data_len = len(trainloader)\n",
        "    p_len = data_len // 5\n",
        "    if (p_len == 0):\n",
        "        p_len = 1\n",
        "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=patience, factor=0.1)\n",
        "    for epoch in (range(epochs)):  # loop over the dataset multiple times\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (printing):\n",
        "                running_loss += loss.item()\n",
        "                if i % p_len == p_len - 1:\n",
        "                    print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / p_len:.3f}')\n",
        "                    running_loss = 0.0\n",
        "        validation_loss = calculate_validation_loss(model, val_loader, criterion)\n",
        "        print(f'validation_loss = {validation_loss}')\n",
        "        scheduler.step(validation_loss)\n",
        "\n",
        "    print('Finished Training')\n",
        "def test(net, test_loader, device='cpu', printing=True):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            # calculate outputs by running images through the network\n",
        "            outputs = net(inputs)\n",
        "            # the class with the highest energy is what we choose as prediction\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accur = correct / total\n",
        "    if (printing):\n",
        "        print(f'Accuracy of the network: {100 * correct / total} %')\n",
        "    return accur"
      ],
      "metadata": {
        "id": "OXTB1eEaqqOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for img, label in new_train_loader:\n",
        "    img, label = img.to(device), label.to(device)\n",
        "    print(img.shape)\n",
        "#     print(img)\n",
        "    print(label.shape)\n",
        "    print(label.size(0))\n",
        "    break\n",
        "for img, label in val_loader:\n",
        "    img, label = img.to(device), label.to(device)\n",
        "    print(img.shape)\n",
        "#     print(img)\n",
        "    print(label.shape)\n",
        "    print(label.size(0))\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgZiWZPcIhv-",
        "outputId": "e6b4cab0-ba2b-49f7-b826-4a2eb9c819b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 1, 28, 28])\n",
            "torch.Size([128])\n",
            "128\n",
            "torch.Size([128, 1, 28, 28])\n",
            "torch.Size([128])\n",
            "128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = FixedNeuralNetworkTmp().to(device)\n",
        "opt = optim.SGD(model.parameters(), lr=1, momentum=0.9, nesterov=True)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "train(model, new_train_loader, val_loader, loss_func, opt, 15, device, patience=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6jxYjfVqrrK",
        "outputId": "c136a128-7638-41a7-c4f0-8a8c95b851f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,    78] loss: 0.817\n",
            "[1,   156] loss: 0.495\n",
            "[1,   234] loss: 0.452\n",
            "[1,   312] loss: 0.435\n",
            "[1,   390] loss: 0.405\n",
            "validation_loss = 0.3966573174995712\n",
            "[2,    78] loss: 0.372\n",
            "[2,   156] loss: 0.386\n",
            "[2,   234] loss: 0.370\n",
            "[2,   312] loss: 0.370\n",
            "[2,   390] loss: 0.354\n",
            "validation_loss = 0.35283039095281044\n",
            "[3,    78] loss: 0.344\n",
            "[3,   156] loss: 0.321\n",
            "[3,   234] loss: 0.325\n",
            "[3,   312] loss: 0.323\n",
            "[3,   390] loss: 0.324\n",
            "validation_loss = 0.35936396163475665\n",
            "[4,    78] loss: 0.307\n",
            "[4,   156] loss: 0.304\n",
            "[4,   234] loss: 0.294\n",
            "[4,   312] loss: 0.298\n",
            "[4,   390] loss: 0.299\n",
            "validation_loss = 0.3619326530378076\n",
            "[5,    78] loss: 0.279\n",
            "[5,   156] loss: 0.288\n",
            "[5,   234] loss: 0.276\n",
            "[5,   312] loss: 0.281\n",
            "[5,   390] loss: 0.273\n",
            "validation_loss = 0.3307560540830033\n",
            "[6,    78] loss: 0.262\n",
            "[6,   156] loss: 0.258\n",
            "[6,   234] loss: 0.266\n",
            "[6,   312] loss: 0.251\n",
            "[6,   390] loss: 0.260\n",
            "validation_loss = 0.30796268318272846\n",
            "[7,    78] loss: 0.246\n",
            "[7,   156] loss: 0.241\n",
            "[7,   234] loss: 0.245\n",
            "[7,   312] loss: 0.243\n",
            "[7,   390] loss: 0.258\n",
            "validation_loss = 0.3093283505096466\n",
            "[8,    78] loss: 0.219\n",
            "[8,   156] loss: 0.230\n",
            "[8,   234] loss: 0.235\n",
            "[8,   312] loss: 0.242\n",
            "[8,   390] loss: 0.241\n",
            "validation_loss = 0.36478971227814877\n",
            "[9,    78] loss: 0.220\n",
            "[9,   156] loss: 0.219\n",
            "[9,   234] loss: 0.229\n",
            "[9,   312] loss: 0.212\n",
            "[9,   390] loss: 0.221\n",
            "validation_loss = 0.289230882508468\n",
            "[10,    78] loss: 0.194\n",
            "[10,   156] loss: 0.203\n",
            "[10,   234] loss: 0.229\n",
            "[10,   312] loss: 0.216\n",
            "[10,   390] loss: 0.209\n",
            "validation_loss = 0.31552887406153013\n",
            "[11,    78] loss: 0.185\n",
            "[11,   156] loss: 0.208\n",
            "[11,   234] loss: 0.196\n",
            "[11,   312] loss: 0.214\n",
            "[11,   390] loss: 0.196\n",
            "validation_loss = 0.3646472223055891\n",
            "[12,    78] loss: 0.186\n",
            "[12,   156] loss: 0.180\n",
            "[12,   234] loss: 0.204\n",
            "[12,   312] loss: 0.189\n",
            "[12,   390] loss: 0.204\n",
            "validation_loss = 0.3023065225916761\n",
            "[13,    78] loss: 0.171\n",
            "[13,   156] loss: 0.182\n",
            "[13,   234] loss: 0.174\n",
            "[13,   312] loss: 0.186\n",
            "[13,   390] loss: 0.179\n",
            "validation_loss = 0.328231740177055\n",
            "[14,    78] loss: 0.165\n",
            "[14,   156] loss: 0.176\n",
            "[14,   234] loss: 0.173\n",
            "[14,   312] loss: 0.178\n",
            "[14,   390] loss: 0.175\n",
            "validation_loss = 0.39455592285700236\n",
            "[15,    78] loss: 0.145\n",
            "[15,   156] loss: 0.128\n",
            "[15,   234] loss: 0.118\n",
            "[15,   312] loss: 0.111\n",
            "[15,   390] loss: 0.114\n",
            "validation_loss = 0.3009722915867084\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(model, test_loader, device, printing=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nUJFCB5sQnQ",
        "outputId": "7a73468c-01b6-4047-938b-a58f42edfd06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network: 89.51 %\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8951"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "На двух слоях $accuracy \\approx  88.5\\%$, но на трех на $\\sim0.5\\%$ больше. Уменьшение lr в процессе обучения дал незначительную прибавку в $\\sim 0.5\\%$."
      ],
      "metadata": {
        "id": "aq3cxejIDKtl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMui_uLJ7G0d"
      },
      "source": [
        "### Conclusions:\n",
        "После всех \"экспериментов\" я думаю, что вариант с двумя слоями и постоянным lr будет наилучшим выбором, потому что прибавку, которую они дают является несущественной. И вообще не стоит переусложнять модель (особенно когда можно сделать проще), ведь переобучить ее становится легче."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t_5W_xVoPn_E"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}